{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8445036-beb6-40d2-8b9c-6bd3c84d937b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 4.0.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Spark Session \n",
    "# Set Java 17 path\n",
    "os.environ[\"JAVA_HOME\"] = \"C:\\\\Program Files\\\\Java\\\\jdk-17\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"\\\\bin;\" + os.environ[\"PATH\"]\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyLocalSparkSession\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark version:\", spark.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39c4afd4-1861-4736-b739-288d59f2703c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "|            Egypt|      United States|   24|\n",
      "|Equatorial Guinea|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Reading the data (With inferschema)\n",
    "df_Flight_Data = spark.read.csv(\"Flight_Data.csv\", header=True, inferSchema=True)\n",
    "df_Flight_Data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58186c91-f8c8-45aa-91d9-2f39d4cc4a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# schema info when infer schema is true \n",
    "df_Flight_Data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e0634a8-598c-4c85-9fad-6621c2287491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|    1|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|              India|   69|\n",
      "|            Egypt|      United States|   24|\n",
      "|Equatorial Guinea|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Reading data without inferschema \n",
    "df_Flight_Data_No_infer = spark.read.csv(\"Flight_Data.csv\", header=True, inferSchema=False)\n",
    "df_Flight_Data_No_infer.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38df69e5-4f35-4cbc-94b4-0484af19115f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Schema when infer is false \n",
    "df_Flight_Data_No_infer.printSchema()\n",
    "# Here you will see the count is string "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c42cfc-58dd-4863-be4d-f688db79564f",
   "metadata": {},
   "source": [
    "# Dealing with Corroupted Record "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe9128d-5371-48ca-b083-4bdf7f1672dc",
   "metadata": {},
   "source": [
    "## Potential Interview Questions:\n",
    "I. Have you worked with corrupted records?\n",
    "\n",
    "II. When do you say that it’s a corrupted record?\n",
    "\n",
    "III. What happens when we encounter corrupted records in different read modes?\n",
    "\n",
    "IV. How can we print bad records?\n",
    "\n",
    "V. Where do you store corrupted records and how can we access them later?) \n",
    "1 and 2) yes, Mostly the bad files are jsoj, csv\n",
    "    Common bad records are missing key value from json file, extra comma or recroed in csv file\n",
    "    value missing etc....\n",
    "3) Permissive mode: Fill null in place of bad record\n",
    "   fail fast: immidiately error out after it gets bad record\n",
    "   DropMalInformed: Drop the courroupted record\n",
    "4) Create schema for bad records that includes all data columns and one more for bad record , and then read the data again with passing that schema\n",
    "5) .option('badrecords', 'Path') - To store the bad record \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e14da25-1ee3-4003-ae85-b9150a920e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
      "+--------------------+-------------------+------+\n",
      "|       United States|            Romania| India|\n",
      "|       United States|            Ireland| India|\n",
      "|       United States|              India| India|\n",
      "|               Egypt|      United States|    24|\n",
      "|   Equatorial Guinea|      United States|     1|\n",
      "|       United States|          Singapore|    25|\n",
      "|       United States|            Grenada|    54|\n",
      "|          Costa Rica|      United States|   477|\n",
      "|             Senegal|      United States|    29|\n",
      "|       United States|   Marshall Islands|    44|\n",
      "|              Guyana|      United States|    17|\n",
      "|       United States|       Sint Maarten|    53|\n",
      "|               Malta|      United States|     1|\n",
      "|             Bolivia|      United States|    46|\n",
      "|            Anguilla|      United States|    21|\n",
      "|Turks and Caicos ...|      United States|   136|\n",
      "|       United States|        Afghanistan|     2|\n",
      "|Saint Vincent and...|      United States|     1|\n",
      "|               Italy|      United States|   390|\n",
      "|       United States|             Russia|   156|\n",
      "+--------------------+-------------------+------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "permissive_flight_data = spark.read.format('csv')\\\n",
    "                        .option('header', 'true')\\\n",
    "                        .option('inferschema' , 'true')\\\n",
    "                        .option('mode','PERMISSIVE')\\\n",
    "                        .load('Flight_Data_Cour.csv')\n",
    "\n",
    "permissive_flight_data.show()\n",
    "\n",
    "# allows the record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da8b7380-22c3-4758-b0e5-28a9f56a7ee1",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o54.showString.\n: org.apache.spark.SparkException: [FAILED_READ_FILE.NO_HINT] Encountered error while reading file file:///C:/Users/shant/Desktop/PySpark-DataBricks/Coading/Flight_Data_Cour.csv.  SQLSTATE: KD001\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:856)\r\n\tat org.apache.spark.sql.execution.datasources.v2.FileDataSourceV2$.attachFilePath(FileDataSourceV2.scala:142)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:142)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:544)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:497)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:58)\r\n\tat org.apache.spark.sql.classic.Dataset.collectFromPlan(Dataset.scala:2244)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$head$1(Dataset.scala:1379)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2234)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2232)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\r\n\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\r\n\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\r\n\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\r\n\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\nCaused by: org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING.WITHOUT_SUGGESTION] Malformed records are detected in record parsing: [United States,Romania, India].\nParse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.  SQLSTATE: 22023\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1525)\r\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.throwMalformedRecordsDetectedInRecordParsingError(FailureSafeParser.scala:92)\r\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:82)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:474)\r\n\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:594)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:608)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext0(FileScanRDD.scala:131)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:292)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext0(FileScanRDD.scala:131)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:140)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\nCaused by: org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: United States,Romania, India,1 SQLSTATE: KD000\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedCSVRecordError(QueryExecutionErrors.scala:1322)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$convert$4(UnivocityParser.scala:350)\r\n\t... 30 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m failfast_flight_data \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcsv\u001b[39m\u001b[38;5;124m'\u001b[39m)\\\n\u001b[0;32m      2\u001b[0m                         \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m'\u001b[39m)\\\n\u001b[0;32m      3\u001b[0m                         \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minferschema\u001b[39m\u001b[38;5;124m'\u001b[39m , \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m'\u001b[39m)\\\n\u001b[0;32m      4\u001b[0m                         \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFAILFAST\u001b[39m\u001b[38;5;124m'\u001b[39m)\\\n\u001b[0;32m      5\u001b[0m                         \u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFlight_Data_Cour.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m failfast_flight_data\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\classic\\dataframe.py:285\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_show_string(n, truncate, vertical))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\classic\\dataframe.py:303\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    298\u001b[0m         errorClass\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    299\u001b[0m         messageParameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    300\u001b[0m     )\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mshowString(n, \u001b[38;5;241m20\u001b[39m, vertical)\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1363\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:282\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpy4j\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotocol\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    284\u001b[0m     converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\protocol.py:327\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    325\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    329\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o54.showString.\n: org.apache.spark.SparkException: [FAILED_READ_FILE.NO_HINT] Encountered error while reading file file:///C:/Users/shant/Desktop/PySpark-DataBricks/Coading/Flight_Data_Cour.csv.  SQLSTATE: KD001\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:856)\r\n\tat org.apache.spark.sql.execution.datasources.v2.FileDataSourceV2$.attachFilePath(FileDataSourceV2.scala:142)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:142)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:544)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:497)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:58)\r\n\tat org.apache.spark.sql.classic.Dataset.collectFromPlan(Dataset.scala:2244)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$head$1(Dataset.scala:1379)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2234)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2232)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\r\n\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\r\n\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\r\n\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\r\n\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\nCaused by: org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING.WITHOUT_SUGGESTION] Malformed records are detected in record parsing: [United States,Romania, India].\nParse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.  SQLSTATE: 22023\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1525)\r\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.throwMalformedRecordsDetectedInRecordParsingError(FailureSafeParser.scala:92)\r\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:82)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:474)\r\n\tat scala.collection.Iterator$$anon$10.nextCur(Iterator.scala:594)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:608)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext0(FileScanRDD.scala:131)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:292)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext0(FileScanRDD.scala:131)\r\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:140)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\nCaused by: org.apache.spark.SparkRuntimeException: [MALFORMED_CSV_RECORD] Malformed CSV record: United States,Romania, India,1 SQLSTATE: KD000\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedCSVRecordError(QueryExecutionErrors.scala:1322)\r\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$convert$4(UnivocityParser.scala:350)\r\n\t... 30 more\r\n"
     ]
    }
   ],
   "source": [
    "failfast_flight_data = spark.read.format('csv')\\\n",
    "                        .option('header', 'true')\\\n",
    "                        .option('inferschema' , 'true')\\\n",
    "                        .option('mode','FAILFAST')\\\n",
    "                        .load('Flight_Data_Cour.csv')\n",
    "\n",
    "failfast_flight_data.show()\n",
    "# fail while loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d4e124-20d7-49df-9f52-0d3937330534",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropmalinformed_flight_data = spark.read.format('csv')\\\n",
    "                        .option('header', 'true')\\\n",
    "                        .option('inferschema' , 'true')\\\n",
    "                        .option(\"mode\", \"DROPMALFORMED\")\\\n",
    "                        .load('Flight_Data_Cour.csv')\n",
    "\n",
    "dropmalinformed_flight_data.show()\n",
    "# drop the miss informed record "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad56aedf-6f7d-479a-affc-b789596ac617",
   "metadata": {},
   "source": [
    "# Transformation and Actions\n",
    "## Potential Interview questions\n",
    "I. What is transformation and how many types of transformation do we have?\n",
    "\n",
    "II. What happens when we use group by or join in transformation?\n",
    "\n",
    "III. How jobs are created in Spark?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfdbb61-ac15-4b19-b0fa-6e0d78ff029d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1) \n",
    "# Transformation : Any process that we perform on data is called transformation \n",
    "# Action: .show(), .count() - kind of calling function\n",
    "# Two type: narrow, wide \n",
    "# narrow - partitions are not dependant on each other, no shuffle, filter, select etc..\n",
    "# wide - when depends on each other, shuffle , group by, join , and it is expensive \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2ca34c-7cdd-473f-8e45-1cb7cae6c1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2) \n",
    "# wide transformation happens ,\n",
    "# it is more expensive\n",
    "# lots of shuffling happens between partitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3e903e-1dd2-4b08-8d79-4233034c9d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3) \n",
    "\"\"\"\n",
    "Bacically action creates job\n",
    "Every action has its seperate job\n",
    "job = \n",
    ".action \n",
    "        - count\n",
    "        - show\n",
    "        - collect\n",
    "            - this creates job \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e6ae4b-b149-4976-8baf-21c4108d8639",
   "metadata": {},
   "source": [
    "# DAG and Lazy Evaluation \n",
    "\n",
    "Directed Ascyclic Graph:  A flow diagram for each job in spark\n",
    "In DAG if it is gray , meaning it is already execuated and dont need to execute again \n",
    "Lazy Evaluation : Before execution each job is evaluated lazily and then out of many plans best plan will be choosen for execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ff6cdb-5466-4a72-b512-5afe863d4e83",
   "metadata": {},
   "source": [
    "# How to read Json file\n",
    "## Potential Interview Questions: \n",
    "I. What is JSON data and how to read it in Spark?\n",
    "\n",
    "II. What if JSON has 3 keys in all lines and 4 keys in one line?\n",
    "\n",
    "III. What is multiline and line-delimited JSON?\n",
    "\n",
    "IV. Which one works faster — multiline or line-delimited?\n",
    "\n",
    "V. How to convert nested JSON into Spark DataFrame?\n",
    "\n",
    "VI. What will happen if JSON has a corrupted JSON file or JSON record?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5460c116-587d-4719-9814-2eb57d1abdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Json data is semi structred data, contains key value pair \n",
    "\n",
    "json_file = spark.read.format(\"json\") \\\n",
    "    .option(\"multiline\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"mode\", \"PERMISSIVE\") \\\n",
    "    .load(\"sample_flights.json\") \n",
    "  \n",
    "\n",
    "json_file.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a76a417-fc81-4cf8-99ab-2d9e49836114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Then it will fail , or depending the mode of the file reader api, it will either drop it, read it or fail at the time of reading the file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b7d08f-04dc-430b-9530-95c88868f6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Line deli - data in single line {} (.option(\"multiline\", \"false\") \\ )\n",
    "#    Multi Line -  data in multiple line  ( .option(\"multiline\", \"true\") \\)\n",
    "\"\"\"\n",
    "example of line deli:\n",
    "{key : value, key : value }\n",
    "example of multi line: \n",
    "{\n",
    "key : value,\n",
    "key : value,\n",
    "key : value\n",
    "}\n",
    "for performance line deli is good, as it reads the data from one line and moves it to anoter line, for multi line , it has to make sure the data is in one {} , so little hard to read \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9b7d5f-bb56-4a0a-a89a-211483dcb9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4),6)  for performance line deli is good, as it reads the data from one line and moves it to anoter line, for multi line , it has to make sure the data is in one {} , so little hard to read \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5530f3-6f03-4ca0-a9bb-b6e584bb7a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5 ) Nested Json reading :\n",
    "\n",
    "Nested_json_file = spark.read.format(\"json\") \\\n",
    "    .option(\"multiline\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"mode\", \"PERMISSIVE\") \\\n",
    "    .load(\"nestedejson.json\") \n",
    "  \n",
    "\n",
    "Nested_json_file.show()\n",
    "Nested_json_file.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d16b6fc-836f-452a-88a5-b28fb2c22816",
   "metadata": {},
   "source": [
    "# SQL Engine\n",
    "\n",
    "## Potential Interview questions\n",
    "\n",
    "I. What is Catalyst Optimizer / Spark SQL Engine?\n",
    "\n",
    "II. Why do we get Analysis Exception error?\n",
    "\n",
    "III. What is Catalog?\n",
    "\n",
    "IV. What is Physical Planning / Spark Plan?\n",
    "\n",
    "V. Is Spark SQL Engine a compiler?\n",
    "\n",
    "VI. How many phases are involved in Spark SQL Engine to convert a code into Java bytecode?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94c7619-6f6a-41c2-a4ec-474fb9ddf48c",
   "metadata": {},
   "source": [
    "I. What is Catalyst Optimizer / Spark SQL Engine?\n",
    "\n",
    "Catalyst Optimizer is the query optimization framework inside Spark SQL.\n",
    "It converts logical plans into optimized physical plans for better performance.\n",
    "It uses techniques like predicate pushdown, constant folding, and join optimization.\n",
    "\n",
    "II. Why do we get Analysis Exception error?\n",
    "\n",
    "This occurs when Spark cannot analyze the logical plan.\n",
    "Common causes: missing columns, invalid paths, unsupported operations, or schema mismatches.\n",
    "It happens before execution, during the query analysis phase.\n",
    "\n",
    "III. What is Catalog?\n",
    "\n",
    "The Catalog stores metadata about databases, tables, columns, and functions in Spark SQL.\n",
    "It helps Spark resolve table names and manage schemas efficiently.\n",
    "You can access it using spark.catalog API.\n",
    "\n",
    "IV. What is Physical Planning / Spark Plan?\n",
    "\n",
    "Physical Planning is the process where Spark decides how to execute a query.\n",
    "The Catalyst Optimizer generates multiple physical plans and picks the most efficient one.\n",
    "You can view it using .explain(mode=\"extended\").\n",
    "\n",
    "V. Is Spark SQL Engine a compiler?\n",
    "\n",
    "Yes ✅ Spark SQL Engine acts like a compiler.\n",
    "It converts high-level queries (SQL or DataFrame API) into optimized RDD operations.\n",
    "Finally, these are compiled into Java bytecode and executed on the cluster.\n",
    "\n",
    "VI. How many phases are involved in Spark SQL Engine to convert code into Java bytecode?\n",
    "\n",
    "There are four main phases:\n",
    "\n",
    "Parsing → converts query into an unresolved logical plan\n",
    "\n",
    "Analysis + Optimization → applies Catalyst rules to optimize\n",
    "\n",
    "Physical Planning & Code Generation → generates Java bytecode for execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643e863e-022d-4754-8ca1-c24699341b95",
   "metadata": {},
   "source": [
    "# RDD\n",
    "Potential Interview questions\n",
    "\n",
    "1) What is RDD\n",
    "2) When do we need RDD\n",
    "3) Features of RDD\n",
    "4) What is dataframe/dataset\n",
    "5) Why we should not use an RDD "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e51c47-4954-4ea0-967e-c084eab90b5d",
   "metadata": {},
   "source": [
    "1) RDD: Rescilient Distrubuted Dataset (Data Structure) (Immutable)\n",
    "   R :  In case of faliure, it can recover (wuth the help of DAG)\n",
    "   D : it is distributed over the cluster\n",
    "   D : Data\n",
    "2) Good for unstructured, type safe (compile time error), when we want full control over our data \n",
    "3) Fault Tolarance, immutable, lazy, optimization\n",
    "4) structured API/Dataset Column row format data - data frame, easy to read \n",
    "5) disadvantage- no optimization done by spark, complex code for RDD , and slow code, less readability\n",
    "\n",
    "RDD - How to ? (Meaning we need to tell RDD, how to do? )\n",
    "Dataframe - What to  ? (Meaning we need to tell dataframe what to do ? it will tell automatically )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f101ad-d522-4b04-bcbc-32026b3d73a7",
   "metadata": {},
   "source": [
    "# Parque File format \n",
    "\n",
    "I. What is Parquet file format? \n",
    "\n",
    "II. Why do we need Parquet? fast, optimized, OLAP, Structured, Encrypted \n",
    "\n",
    "III. How to read Parquet file? \n",
    "\n",
    "IV. What makes Parquet a default choice?\n",
    "\n",
    "V. What encoding is done on data? RLP (Run lenght encoding) (will convert aaaaaavvvvvvbbce to a6v6b2c1e1 to make it faster)\n",
    "\n",
    "VI. What compression techniques are used? GZIP, SNAPPY\n",
    "\n",
    "VII. How to optimize the Parquet file? Use Analyze command \n",
    "\n",
    "VIII. What is row group, column, and pages ?  after reading file it will automatically group the data in chunk of row, column and these row and column chunk are in page\n",
    "\n",
    "IX. How projection Pruning and predicate pushdown works ? \n",
    "\n",
    "Predicate Pushdown: Spark optimization where filter conditions are pushed down to the data source (e.g., Parquet, ORC) instead of filtering after data is loaded.\n",
    "Projection Pruning : The columns which are not required are not involved or called (select id where age< 19 , so if in a group <19 is not available it will stop to scan the data  )\n",
    "\n",
    "1) It is a file format, columanar based file format, it is structured file format, and it is in binary type. (Actually in hybrid not row or columnar)\n",
    "   OLAP - Online Analytical processing (Used for analytical , as there is less need to update) (Columnar file format)\n",
    "   OLTP - Online Transcation Processing (Used where we need to update the record), (Row based file format)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283c1f15-6e65-4fd8-a72d-20d418ea78e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read parquet file\n",
    "\n",
    "df_parquet = spark.read.parquet('part-r-00000-1a9822ba-b8fb-4d8e-844a-ea30d0801b9e.gz.parquet')\n",
    "df_parquet.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66240da-33ed-48f5-885f-2fb3535eb746",
   "metadata": {},
   "source": [
    "# How to write data in spark ? \n",
    "## Potential Interview questions \n",
    "\n",
    "1) What are the modes available in dataframe writer?\n",
    "2) What is partitionBy and bucketBy?\n",
    "      A. WWhy we need these two ?\n",
    "      B. When to use whicn ? \n",
    "4) How to write data into multiple partitions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010f8029-c358-450c-a432-5c113cdbad2e",
   "metadata": {},
   "source": [
    "The general structure of how to write data in spark is \n",
    "df = spark.write.format(\"mention your format\")\\\n",
    "    .option('header','true')\\\n",
    "    .option('mode', 'mention mode ')\\\n",
    "    .partitionBy('')\\\n",
    "    .bucketBy('')\\\n",
    "    .option('path','----')\\\n",
    "    .save()\n",
    "\n",
    "\n",
    "1) Modes are append, overwrite, errorifexist, ignore\n",
    "   1) Append: It will add the file in the location\n",
    "   2) Overwrite:  delete the first file and saves the new file\n",
    "   3) ErrorIfExist : check if location or file exists at the same location, if the file exists in the location it throws error(file exists/location dosent exist)\n",
    "   4) Ignore: It will keep the existing file in the location and will not write new file (basically it ignores the new file and keep the existing one)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab513de-0473-4af7-b06a-61695753c23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example:\n",
    "\n",
    "# first lets read the file\n",
    "\n",
    "employee_df = spark.read.format('csv')\\\n",
    "            .option('header','true')\\\n",
    "            .option('inferschema','true')\\\n",
    "            .option('mode','PERMISSIVE')\\\n",
    "            .load('employee_data.csv')\n",
    "\n",
    "employee_df.show(20)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "76913c3a-7747-45e6-ae6d-449db0e63888",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "writeEmpdf = employee_df.write.format('csv') \\\n",
    "    .option('header', 'true') \\\n",
    "    .option('mode', 'overwrite') \\\n",
    "    .option('path', 'writtenFile.csv') \\\n",
    "    .save()\n",
    "\n",
    "# for some reason this is throwing error, i need to look at it , but for now that is how we write a file \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7e6066-2744-4ec6-ad37-b9b2a3495bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition Example\n",
    "# writeEmpdf = employee_df.write.format('csv') \\\n",
    "#     .option('header', 'true') \\\n",
    "#     .option('mode', 'overwrite') \\\n",
    "#     .option('path', 'writtenFile.csv') \\\n",
    "#     .partitionBy('ColumnName') \\\n",
    "#     .save()\n",
    "\n",
    "# Advantages of Partition:\n",
    "# 1) No need to scan the full data.\n",
    "# 2) Helps scan data in chunks.\n",
    "# 3) Fails if the partition column has very few distinct values (uneven partitioning).\n",
    "# 4) Causes shuffling of data during write.\n",
    "\n",
    "# Bucket Example\n",
    "# writeEmpdf = employee_df.write \\\n",
    "#     .bucketBy(4, 'ColumnName') \\\n",
    "#     .sortBy('ColumnName') \\\n",
    "#     .option('header', 'true') \\\n",
    "#     .option('mode', 'overwrite') \\\n",
    "#     .saveAsTable('BucketedTable')\n",
    "\n",
    "# Advantages of Bucketing:\n",
    "# 1) No shuffling during write.\n",
    "# 2) Faster for joins and aggregations.\n",
    "\n",
    "# Partition vs Bucketing:\n",
    "# Partition By                   | Bucket By\n",
    "# ------------------------------ | ------------------------------\n",
    "# Divides data by actual values  | Divides data by hash of column\n",
    "# Creates dynamic number of parts| Creates fixed number of buckets\n",
    "# Creates separate folders       | All bucket files stored in one folder\n",
    "# Best for filter-heavy queries  | Best for join-heavy queries"
   ]
  },
  {
   "cell_type": "raw",
   "id": "03c7d894-3fbd-4a1d-b95c-f212180e9c36",
   "metadata": {},
   "source": [
    "# Writing file into multiple partitions manually using repartition()\n",
    "\n",
    "writeEmpdf = employee_df.repartition(3).write.format('csv') \\\n",
    "    .option('header', 'true') \\\n",
    "    .option('mode', 'overwrite') \\\n",
    "    .option('path', 'writtenFile.csv') \\\n",
    "    .save()\n",
    "\n",
    "# Explanation:\n",
    "# 1) repartition(3) → divides the DataFrame into exactly 3 partitions.\n",
    "# 2) Spark will create 3 separate CSV files (part-00000, part-00001, part-00002).\n",
    "# 3) Each file will contain a portion of the data.\n",
    "# 4) This is useful for parallelism and controlling file sizes.\n",
    "\n",
    "# Final Output:\n",
    "# 📂 writtenFile.csv\n",
    "#    ├── part-00000-xxxxx.csv\n",
    "#    ├── part-00001-xxxxx.csv\n",
    "#    └── part-00002-xxxxx.csv\n",
    "\n",
    "# Done! ✅ Now the data is written into 3 different partitions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38046b3-e4ee-4043-a1cc-c203b2b470bc",
   "metadata": {},
   "source": [
    "# Spark Session Vs Spark Context \n",
    "\n",
    "Both provies entry to spark session.\n",
    "\n",
    "before spark 1.0\n",
    "for everything we need to create context \n",
    "example\n",
    "1) sql - sql context\n",
    "2) spark - sparl context\n",
    "3) hive - hive context \n",
    "\n",
    "now after 1.0 everything is in spark session, no need to do seperately "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cf8c06-9399-4e2f-a943-3062b8d9588b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Job, Stage, Task, Applicaion \n",
    "\n",
    "Read the book "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f5f1b8-98d8-4e7d-94f0-acbb98dd2851",
   "metadata": {},
   "source": [
    "# How to create datafram in spark ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb3e772-e789-469f-ad07-04793af8547e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType\n",
    "\n",
    "my_data = [(1,2),(1,2),(1,2),(1,2),(1,2),(1,2)]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"ID\",  IntegerType(), True),\n",
    "    StructField(\"Num\", IntegerType(), True),\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(my_data, schema)\n",
    "df.show()\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a106427-76d3-48d9-8c6b-8788e9350722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Your data\n",
    "my_data = [\n",
    "    (1, 2),\n",
    "    (1, 2),\n",
    "    (1, 2),\n",
    "    (1, 2),\n",
    "    (1, 2),\n",
    "    (1, 2)\n",
    "]\n",
    "\n",
    "# ✅ Create DataFrame directly & rename columns\n",
    "create_df = spark.createDataFrame(my_data).toDF(\"ID\", \"Num\")\n",
    "\n",
    "# ✅ Show DataFrame content\n",
    "create_df.show()\n",
    "\n",
    "# ✅ Print schema (optional)\n",
    "create_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad1dd08-19e9-4499-a938-5c0397bfc93e",
   "metadata": {},
   "source": [
    "# Hands On Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26b40b7-47af-4e44-bbed-94413bd9aa35",
   "metadata": {},
   "source": [
    "## Lecture 1\n",
    "- What is schema? - column name , data type\n",
    "- What is dataframe? - tabular form of data , it is made of two things row and columns\n",
    "- How to select columns? \n",
    "- How many ways to select columns?\n",
    "- What is expression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453315df-305a-4a6d-ab24-961e24704652",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Flight_Data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4a0800-bf49-4151-967a-0e4fe8219a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for now we will select flight data, as above cellse are getting error \n",
    "# how to select columns\n",
    "df_Flight_Data.columns\n",
    "df_Flight_Data.select(\"count\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98964add-7df6-4808-8f69-df24a17d4d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple ways to selecting columns\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    " \n",
    "df_Flight_Data.select(\"count\").show(5) # String method\n",
    "df_Flight_Data.select(col('count')).show(5) # col method\n",
    "df_Flight_Data.select(col('count')+5).show(5)\n",
    "df_Flight_Data.select(col('count') ,col('DEST_COUNTRY_NAME')).show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc5a082-d2a2-4664-bf1d-1c9f656e54ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# expression\n",
    "\n",
    "# It lets you write SQL-like expressions inside PySpark code.\n",
    "#df_Flight_Data.select(expr('DEST_COUNTRY_NAME'+'HIIII')).show(5) # getting analysis expression, so filing at catalyst level \n",
    "\n",
    "\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "df_Flight_Data.select(expr(\"DEST_COUNTRY_NAME || ' HIIII'\")).show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7da3264-c09e-44ff-9c8d-a50895f2bb0d",
   "metadata": {},
   "source": [
    "## Lecture 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfdf5b5-1020-4f08-87c3-16a298405c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import lit\n",
    "# Alies\n",
    "df_Flight_Data.select(col('count').alias('Total Count')).show()\n",
    "\n",
    "# Filter columns\n",
    "df_Flight_Data.filter(col('DEST_COUNTRY_NAME') == 'United States').show()\n",
    "\n",
    "#literal\n",
    "df_Flight_Data.select('*', lit('Economy').alias('class')).show()\n",
    "\n",
    "#Adding columns:\n",
    "new_df = df_Flight_Data.withColumn('Price',lit('90000')).show()\n",
    "\n",
    "# Rename Columns:\n",
    "df_Flight_Data.withColumnRenamed('Count','Trip').show()\n",
    "\n",
    "# Type casting\n",
    "df_Flight_Data.withColumn('count',col('count').cast(\"string\")).printSchema()\n",
    "\n",
    "# Removing columns\n",
    "df_Flight_Data.drop(col('count')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20482368-8999-4d91-9531-af47c8c333da",
   "metadata": {},
   "source": [
    "# Lecture 3 \n",
    "Union vs Union All "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa67bc4-41db-4ef8-a89f-83a1699abf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Create two simple DataFrames\n",
    "data1 = [(1, \"India\"), (2, \"USA\"), (3, \"UK\")]\n",
    "data2 = [(3, \"UK\"), (4, \"Canada\"), (5, \"Germany\")]\n",
    "columns = [\"ID\", \"Country\"]\n",
    "\n",
    "df1 = spark.createDataFrame(data1, columns)\n",
    "df2 = spark.createDataFrame(data2, columns)\n",
    "\n",
    "# ✅ UNION → Removes duplicates\n",
    "df_union = df1.union(df2)\n",
    "print(\"=== UNION (Removes duplicates) ===\")\n",
    "df_union.show()\n",
    "\n",
    "# ✅ UNION ALL → Keeps duplicates (deprecated in Spark 3.x, use union instead)\n",
    "df_union_all = df1.unionAll(df2)  # For Spark 3+, df1.union(df2)\n",
    "print(\"=== UNION ALL (Keeps duplicates) ===\")\n",
    "df_union_all.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bee134d1-d3b7-45a0-b7c8-8c91bc646c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Potential Interview Questions :\n",
    "\n",
    "# 1️⃣ Difference between union and unionAll:\n",
    "# ------------------------------------------------\n",
    "# ➡ union      → Combines two DataFrames and REMOVES duplicate rows.\n",
    "# ➡ unionAll   → Combines two DataFrames and KEEPS duplicate rows.\n",
    "# ⚡ Note: In Spark 3.x, unionAll() is deprecated, and union() behaves like unionAll() by default.\n",
    "#\n",
    "# Example:\n",
    "# df1.union(df2)           # Removes duplicates in older versions (<3.0)\n",
    "# df1.unionAll(df2)        # Keeps duplicates (deprecated in Spark 3.x)\n",
    "# In Spark 3.x, use:\n",
    "# df1.union(df2)           # Always keeps duplicates unless you add .distinct()\n",
    "\n",
    "# 2️⃣ What will happen if the number of columns is different while unioning?\n",
    "# --------------------------------------------------------------------------\n",
    "# ❌ Spark will throw an AnalysisException if the number of columns in df1 and df2 don't match.\n",
    "#\n",
    "# Example:\n",
    "# df1 has 2 columns (ID, Name)\n",
    "# df2 has 3 columns (ID, Name, Age)\n",
    "# df1.union(df2)   # ❌ Will fail with: \"Union can only be performed on tables with the same number of columns\"\n",
    "\n",
    "# 3️⃣ What if column names are different?\n",
    "# ---------------------------------------\n",
    "# ✅ Column names DON'T matter for union / unionAll.\n",
    "# Spark unions based on POSITION, not NAME.\n",
    "#\n",
    "# Example:\n",
    "# df1 columns → [\"ID\", \"Name\"]\n",
    "# df2 columns → [\"UserID\", \"FullName\"]\n",
    "# df1.union(df2)  ✅ Works fine, but column order must match.\n",
    "\n",
    "# 4️⃣ What is unionByName?\n",
    "# -------------------------\n",
    "# ➡ unionByName matches columns by NAME instead of POSITION.\n",
    "# This is useful when column names are the same but in a different order.\n",
    "#\n",
    "# Example:\n",
    "# df1 → columns: [\"ID\", \"Name\"]\n",
    "# df2 → columns: [\"Name\", \"ID\"]\n",
    "#\n",
    "# df1.unionByName(df2) ✅ Works perfectly.\n",
    "# Spark aligns columns by name internally.\n",
    "#\n",
    "# ⚡ unionByName(ignoreNulls=True) can also handle missing columns gracefully by filling them with nulls.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e55748-d1f6-4837-ae32-70ed6a61abd2",
   "metadata": {},
   "source": [
    "# Lecture 4\n",
    "## Repartation and Coalesce\n",
    "\n",
    "## Potentail Interview Questions \n",
    "\n",
    "1) What is reapartation?\n",
    "2) What is coalesce\n",
    "3) Which one will you choose and why ?\n",
    "4) Difference "
   ]
  },
  {
   "cell_type": "raw",
   "id": "4588bbbf-367e-4122-a7b0-7d327ad8ac25",
   "metadata": {},
   "source": [
    "Ans: \n",
    "First why we need it ? \n",
    "Issue : \n",
    "Read the book for the same.\n",
    "\n",
    "1) Repartition will try to make the partition even, so that the data will process in same time. \n",
    "200 MB will be divided in 50 - 4 each\n",
    "\n",
    "2) Coalesce: It will mereg the partition. It will done the mereging in way it will see which two/three can be merged \n",
    "200 MB will be merged in 50, 100, 50 \n",
    "\n",
    "\n",
    "3) when to use Re: when data need to be evenly distrubuted\n",
    "   when to use Co: when partitions need lesser\n",
    "\n",
    "4)\n",
    "Repartition                              Coalesec\n",
    "Shuffeling                              No shuffeling\n",
    "\n",
    "Expensive                              less expensive\n",
    "Data evenly dist                        NA\n",
    "\n",
    "More IO as more shufeling                lesss IO as less sheffeling\n",
    "Can Increase or decrease size           fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23365ddd-b114-490f-8dc4-4d8cc28f41d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|    1|\n",
      "|       United States|            Ireland|  264|\n",
      "|       United States|              India|   69|\n",
      "|               Egypt|      United States|   24|\n",
      "|   Equatorial Guinea|      United States|    1|\n",
      "|       United States|          Singapore|   25|\n",
      "|       United States|            Grenada|   54|\n",
      "|          Costa Rica|      United States|  477|\n",
      "|             Senegal|      United States|   29|\n",
      "|       United States|   Marshall Islands|   44|\n",
      "|              Guyana|      United States|   17|\n",
      "|       United States|       Sint Maarten|   53|\n",
      "|               Malta|      United States|    1|\n",
      "|             Bolivia|      United States|   46|\n",
      "|            Anguilla|      United States|   21|\n",
      "|Turks and Caicos ...|      United States|  136|\n",
      "|       United States|        Afghanistan|    2|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|               Italy|      United States|  390|\n",
      "|       United States|             Russia|  156|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# Example: \n",
    "\n",
    "df_Flight_Data.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9b9fdb1-36ac-43e1-9e37-459c08937a4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# partition can be done but first , df need to be converted to RDD then repartatiojn is done \n",
    "# df -------------> RDD ----------------> Repartition \n",
    "# how ? \n",
    "\n",
    "df_Flight_Data.rdd.getNumPartitions() # this will give number of partition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cdb3261-5a7c-43e9-9fab-31c2b3734e2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets repartition \n",
    "df_Flight_Data_in_3_partition = df_Flight_Data.repartition(3)\n",
    "df_Flight_Data_in_3_partition.rdd.getNumPartitions() # now data is in 3 partition \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0866d5c4-cbdd-4eb8-b237-668a114befdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|partitionID|count|\n",
      "+-----------+-----+\n",
      "|          0|   85|\n",
      "|          1|   85|\n",
      "|          2|   85|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lets see how the data is devided in the partition\n",
    "from pyspark.sql.functions import spark_partition_id, expr\n",
    "\n",
    "df_Flight_Data_in_3_partition.withColumn('partitionID',spark_partition_id()).groupby('partitionID').count().show()\n",
    "# so when done repartition the data is evenly distributed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e8eba55-ca2c-4fcc-92e5-2e19044dc6d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Coalesec\n",
    "# first lets do repartation into 8 then merge them in 3 \n",
    "df_Flight_Data.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a001097-251b-4337-a94f-d8e8904d1430",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Flight_Data_in_8_partitions = df_Flight_Data.repartition(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96ded9ac-f573-4db4-849a-741668806bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|partitio_id|count|\n",
      "+-----------+-----+\n",
      "|          0|   32|\n",
      "|          1|   31|\n",
      "|          2|   32|\n",
      "|          3|   32|\n",
      "|          4|   32|\n",
      "|          5|   32|\n",
      "|          6|   32|\n",
      "|          7|   32|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_Flight_Data_in_8_partitions.withColumn('partitio_id',spark_partition_id()).groupby('partitio_id').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5346a0a0-97cf-46fc-9842-54fa9aef6093",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Flight_Data_in_8_partitions_in_3_coalesce=df_Flight_Data_in_8_partitions.coalesce(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "edc70565-9b19-4de6-92a0-ee3ed0cccfee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|Partition_id|count|\n",
      "+------------+-----+\n",
      "|           0|   64|\n",
      "|           1|   95|\n",
      "|           2|   96|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_Flight_Data_in_8_partitions_in_3_coalesce.withColumn('Partition_id',spark_partition_id()).groupby('Partition_id').count().show()\n",
    "# so this is not even"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e96f70-dd24-4454-b167-5e34433f4327",
   "metadata": {},
   "source": [
    "# Lecture 5\n",
    "## If-else/when-otherwise/case when \n",
    "\n",
    "## potential interview questions\n",
    "1) case when in saprk\n",
    "2) otherwise in saprk\n",
    "3) how to deal with null values\n",
    "4) case when , if else with multiple and or conditions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3e377ad-0e41-4856-905c-805e287408a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----+-----------------+--------------+--------+-------------------+\n",
      "|           name| age|           gender|       country|  salary|         occupation|\n",
      "+---------------+----+-----------------+--------------+--------+-------------------+\n",
      "|    Casey Patel|NULL|           Female|          NULL| 74144.0|   Business Analyst|\n",
      "|      Casey Lee|55.0|             Male|        Mexico|120302.0|  Financial Analyst|\n",
      "|   Alex Johnson|34.0|           Female|         Japan| 99453.0|       Data Analyst|\n",
      "|   Reese Garcia|62.0|Prefer not to say|United Kingdom| 89439.0|       Data Analyst|\n",
      "| Cameron Harris|21.0|           Female|     Australia|109597.0|Mechanical Engineer|\n",
      "|   Riley Garcia|42.0|             NULL|         India| 79898.0|         Accountant|\n",
      "|   Jessie Clark|43.0|       Non-binary| United States|175647.0|  Software Engineer|\n",
      "| Reese Williams|45.0|             Male|         Japan|    NULL|     Data Scientist|\n",
      "|Shanaya Jackson|44.0|             NULL|         India| 53003.0|      HR Specialist|\n",
      "|    Avery White|NULL|             Male|United Kingdom|    NULL|   Business Analyst|\n",
      "|           NULL|50.0|       Non-binary|        Canada|128520.0|  Software Engineer|\n",
      "|      Jamie Lee|38.0|             Male|        Mexico|143227.0|      Sales Manager|\n",
      "| Reese Thompson|36.0|           Female|          NULL| 79867.0|              Nurse|\n",
      "|  Sophia Nguyen|65.0|           Female|        France|110237.0|         Accountant|\n",
      "|   Olivia Smith|35.0|             Male|        France| 86290.0|               NULL|\n",
      "|  Taylor Garcia|57.0|       Non-binary|United Kingdom| 97954.0|               NULL|\n",
      "|  Logan Ramirez|62.0|Prefer not to say|        Canada|124718.0|     Data Scientist|\n",
      "| Avery Thompson|56.0|       Non-binary|        Mexico| 83077.0|              Nurse|\n",
      "|   Skyler Lopez|44.0|           Female|        Canada| 78392.0| Research Assistant|\n",
      "|   Taylor White|24.0|             Male|        Canada|172240.0|     Data Scientist|\n",
      "+---------------+----+-----------------+--------------+--------+-------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# lets read the data\n",
    "people_df = spark.read.format('csv')\\\n",
    "            .option('header','true')\\\n",
    "            .option('inferschema','true')\\\n",
    "            .option('mode','PERMISSIVE')\\\n",
    "            .load('people_data.csv')\n",
    "people_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7d10ed3-6b27-472e-824e-bf4d6cf32bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----+-----------------+--------------+--------+-------------------+---------+\n",
      "|           name| age|           gender|       country|  salary|         occupation|    Adult|\n",
      "+---------------+----+-----------------+--------------+--------+-------------------+---------+\n",
      "|    Casey Patel|NULL|           Female|          NULL| 74144.0|   Business Analyst|Bad Value|\n",
      "|      Casey Lee|55.0|             Male|        Mexico|120302.0|  Financial Analyst|      YES|\n",
      "|   Alex Johnson|34.0|           Female|         Japan| 99453.0|       Data Analyst|      YES|\n",
      "|   Reese Garcia|62.0|Prefer not to say|United Kingdom| 89439.0|       Data Analyst|      YES|\n",
      "| Cameron Harris|21.0|           Female|     Australia|109597.0|Mechanical Engineer|       NO|\n",
      "|   Riley Garcia|42.0|             NULL|         India| 79898.0|         Accountant|      YES|\n",
      "|   Jessie Clark|43.0|       Non-binary| United States|175647.0|  Software Engineer|      YES|\n",
      "| Reese Williams|45.0|             Male|         Japan|    NULL|     Data Scientist|      YES|\n",
      "|Shanaya Jackson|44.0|             NULL|         India| 53003.0|      HR Specialist|      YES|\n",
      "|    Avery White|NULL|             Male|United Kingdom|    NULL|   Business Analyst|Bad Value|\n",
      "|           NULL|50.0|       Non-binary|        Canada|128520.0|  Software Engineer|      YES|\n",
      "|      Jamie Lee|38.0|             Male|        Mexico|143227.0|      Sales Manager|      YES|\n",
      "| Reese Thompson|36.0|           Female|          NULL| 79867.0|              Nurse|      YES|\n",
      "|  Sophia Nguyen|65.0|           Female|        France|110237.0|         Accountant|      YES|\n",
      "|   Olivia Smith|35.0|             Male|        France| 86290.0|               NULL|      YES|\n",
      "|  Taylor Garcia|57.0|       Non-binary|United Kingdom| 97954.0|               NULL|      YES|\n",
      "|  Logan Ramirez|62.0|Prefer not to say|        Canada|124718.0|     Data Scientist|      YES|\n",
      "| Avery Thompson|56.0|       Non-binary|        Mexico| 83077.0|              Nurse|      YES|\n",
      "|   Skyler Lopez|44.0|           Female|        Canada| 78392.0| Research Assistant|      YES|\n",
      "|   Taylor White|24.0|             Male|        Canada|172240.0|     Data Scientist|       NO|\n",
      "+---------------+----+-----------------+--------------+--------+-------------------+---------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# if age is > 18 adult or else no\n",
    "from pyspark.sql.functions import when, col  # (optionally lit)\n",
    "# 1,2) \n",
    "people_df.withColumn('Adult',when(col('age')>25,'YES')\n",
    "                            .when(col('age')<25,'NO')\n",
    "                            .otherwise('Bad Value')).show()\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6639cc6-1abb-48fa-8c65-7041bea78a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----+-----------------+--------------+--------+-------------------+\n",
      "|           name| age|           gender|       country|  salary|         occupation|\n",
      "+---------------+----+-----------------+--------------+--------+-------------------+\n",
      "|    Casey Patel|19.0|           Female|          NULL| 74144.0|   Business Analyst|\n",
      "|      Casey Lee|55.0|             Male|        Mexico|120302.0|  Financial Analyst|\n",
      "|   Alex Johnson|34.0|           Female|         Japan| 99453.0|       Data Analyst|\n",
      "|   Reese Garcia|62.0|Prefer not to say|United Kingdom| 89439.0|       Data Analyst|\n",
      "| Cameron Harris|21.0|           Female|     Australia|109597.0|Mechanical Engineer|\n",
      "|   Riley Garcia|42.0|             NULL|         India| 79898.0|         Accountant|\n",
      "|   Jessie Clark|43.0|       Non-binary| United States|175647.0|  Software Engineer|\n",
      "| Reese Williams|45.0|             Male|         Japan|    NULL|     Data Scientist|\n",
      "|Shanaya Jackson|44.0|             NULL|         India| 53003.0|      HR Specialist|\n",
      "|    Avery White|19.0|             Male|United Kingdom|    NULL|   Business Analyst|\n",
      "|           NULL|50.0|       Non-binary|        Canada|128520.0|  Software Engineer|\n",
      "|      Jamie Lee|38.0|             Male|        Mexico|143227.0|      Sales Manager|\n",
      "| Reese Thompson|36.0|           Female|          NULL| 79867.0|              Nurse|\n",
      "|  Sophia Nguyen|65.0|           Female|        France|110237.0|         Accountant|\n",
      "|   Olivia Smith|35.0|             Male|        France| 86290.0|               NULL|\n",
      "|  Taylor Garcia|57.0|       Non-binary|United Kingdom| 97954.0|               NULL|\n",
      "|  Logan Ramirez|62.0|Prefer not to say|        Canada|124718.0|     Data Scientist|\n",
      "| Avery Thompson|56.0|       Non-binary|        Mexico| 83077.0|              Nurse|\n",
      "|   Skyler Lopez|44.0|           Female|        Canada| 78392.0| Research Assistant|\n",
      "|   Taylor White|24.0|             Male|        Canada|172240.0|     Data Scientist|\n",
      "+---------------+----+-----------------+--------------+--------+-------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "#3) Null valuse \n",
    "people_df.withColumn('age', when(col('age').isNull(), 19).otherwise(col('age'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c3d24ae-24fb-4fba-8f94-2fd050b323d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Column.otherwise() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#4) \u001b[39;00m\n\u001b[0;32m      2\u001b[0m people_df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSenority\u001b[39m\u001b[38;5;124m'\u001b[39m,when((col(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m18\u001b[39m) \u001b[38;5;241m&\u001b[39m (col(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m30\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdult\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m                                \u001b[38;5;241m.\u001b[39mwhen((col(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m30\u001b[39m)\u001b[38;5;241m&\u001b[39m(col(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m60\u001b[39m),\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m                                \u001b[38;5;241m.\u001b[39motherwise(col(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m'\u001b[39m),\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNot Applicable\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;241m.\u001b[39mshow\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\errors\\utils.py:285\u001b[0m, in \u001b[0;36m_with_origin.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    282\u001b[0m jvm_pyspark_origin\u001b[38;5;241m.\u001b[39mset(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, _capture_call_site(depth))\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    287\u001b[0m     jvm_pyspark_origin\u001b[38;5;241m.\u001b[39mclear()\n",
      "\u001b[1;31mTypeError\u001b[0m: Column.otherwise() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "#4) \n",
    "people_df.withColumn('Senority',when((col('age')>18) & (col('age')<30), 'Adult')\n",
    "                               .when((col('age')>30)&(col('age')<60),'Mid')\n",
    "                               .otherwise(col('age'),'Not Applicable')).show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c24163e-9fc3-4e94-b4d3-c4c836afa851",
   "metadata": {},
   "source": [
    "# Lecture 6\n",
    "## Unique and Sorted Records \n",
    "\n",
    "## Potential Interview Questions \n",
    "\n",
    "1) How to find unique rows ?\n",
    "2) How to drop duplicate ?\n",
    "3) How to sort data ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e3f359-1e66-48cc-8ded-f85aae70493d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1) \n",
    "people_df.distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466e1895-cada-4a7e-8277-d1a152fe8fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "people_df.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a02093c-9219-4471-8fcc-dc242ef05d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2)\n",
    "people_df.drop_duplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cff227b-3392-41dc-9164-60939a0b11cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3) \n",
    "people_df.sort(col('salary').desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c63183-adcd-43c5-8661-daf1212a38f6",
   "metadata": {},
   "source": [
    "# Lecture 7 \n",
    "## Agg funcitons\n",
    "\n",
    "## Potential Int questions \n",
    "1) count  -- count is both action and transformation (interview tip )\n",
    "2) min\n",
    "3) max\n",
    "4) avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99760a10-07cd-4900-b819-a273c8a77bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "people_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d9e328c4-eff8-4f3c-81ff-0c1f15e0b040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|max(salary)|\n",
      "+-----------+\n",
      "|   175647.0|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, col  # (optionally lit)\n",
    "\n",
    "\n",
    "people_df.agg({\"salary\": \"max\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f5014075-3dd0-4e55-923f-55706514881d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------------+-----------------+\n",
      "|Maximum Sal|Sum Of Salary|Minimum salary|   Average Salary|\n",
      "+-----------+-------------+--------------+-----------------+\n",
      "|   175647.0|    8000392.0|       46624.0|95242.76190476191|\n",
      "+-----------+-------------+--------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max, sum, min, avg\n",
    "people_df.select(max('salary').alias('Maximum Sal'), sum('salary').alias('Sum Of Salary'),min('salary').alias('Minimum salary'), avg('salary').alias('Average Salary')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692d1525-9e71-43af-944e-4d7537268f7a",
   "metadata": {},
   "source": [
    "# Lecture 8\n",
    "## Group By\n",
    "## Potential Interview questions\n",
    "\n",
    "1) How group by works ? - groups the data based on the mentioned column\n",
    "2) how to implement it in spark ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48dcbb4b-9488-4073-a540-6a3499ed3ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----+-----------------+--------------+--------+-------------------+\n",
      "|           name| age|           gender|       country|  salary|         occupation|\n",
      "+---------------+----+-----------------+--------------+--------+-------------------+\n",
      "|    Casey Patel|NULL|           Female|          NULL| 74144.0|   Business Analyst|\n",
      "|      Casey Lee|55.0|             Male|        Mexico|120302.0|  Financial Analyst|\n",
      "|   Alex Johnson|34.0|           Female|         Japan| 99453.0|       Data Analyst|\n",
      "|   Reese Garcia|62.0|Prefer not to say|United Kingdom| 89439.0|       Data Analyst|\n",
      "| Cameron Harris|21.0|           Female|     Australia|109597.0|Mechanical Engineer|\n",
      "|   Riley Garcia|42.0|             NULL|         India| 79898.0|         Accountant|\n",
      "|   Jessie Clark|43.0|       Non-binary| United States|175647.0|  Software Engineer|\n",
      "| Reese Williams|45.0|             Male|         Japan|    NULL|     Data Scientist|\n",
      "|Shanaya Jackson|44.0|             NULL|         India| 53003.0|      HR Specialist|\n",
      "|    Avery White|NULL|             Male|United Kingdom|    NULL|   Business Analyst|\n",
      "|           NULL|50.0|       Non-binary|        Canada|128520.0|  Software Engineer|\n",
      "|      Jamie Lee|38.0|             Male|        Mexico|143227.0|      Sales Manager|\n",
      "| Reese Thompson|36.0|           Female|          NULL| 79867.0|              Nurse|\n",
      "|  Sophia Nguyen|65.0|           Female|        France|110237.0|         Accountant|\n",
      "|   Olivia Smith|35.0|             Male|        France| 86290.0|               NULL|\n",
      "|  Taylor Garcia|57.0|       Non-binary|United Kingdom| 97954.0|               NULL|\n",
      "|  Logan Ramirez|62.0|Prefer not to say|        Canada|124718.0|     Data Scientist|\n",
      "| Avery Thompson|56.0|       Non-binary|        Mexico| 83077.0|              Nurse|\n",
      "|   Skyler Lopez|44.0|           Female|        Canada| 78392.0| Research Assistant|\n",
      "|   Taylor White|24.0|             Male|        Canada|172240.0|     Data Scientist|\n",
      "+---------------+----+-----------------+--------------+--------+-------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "people_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "30887a08-2b9c-4b5d-9d33-44ed7b842622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|          occupation|sum(salary)|\n",
      "+--------------------+-----------+\n",
      "|   Financial Analyst|   324887.0|\n",
      "|               Nurse|   491628.0|\n",
      "|             Teacher|   571511.0|\n",
      "|                NULL|   471463.0|\n",
      "|Marketing Specialist|   177853.0|\n",
      "|    Business Analyst|   584234.0|\n",
      "|      Data Scientist|   872635.0|\n",
      "|  Operations Analyst|   269706.0|\n",
      "|        Data Analyst|   526066.0|\n",
      "|       HR Specialist|   454296.0|\n",
      "|       Sales Manager|   348607.0|\n",
      "| Mechanical Engineer|   367932.0|\n",
      "|          Accountant|   555216.0|\n",
      "|  Research Assistant|   501655.0|\n",
      "|   Software Engineer|   838459.0|\n",
      "|     Product Manager|   644244.0|\n",
      "+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# group by occupation and take the sum of salary\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "people_df.groupby('occupation').agg(sum('salary')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "415960a8-3478-48ad-b282-2085be5bf14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+-----------+\n",
      "|       country|          occupation|sum(salary)|\n",
      "+--------------+--------------------+-----------+\n",
      "|        Mexico|          Accountant|   119477.0|\n",
      "|        Mexico|       Sales Manager|   143227.0|\n",
      "|     Australia|       HR Specialist|   101495.0|\n",
      "|       Germany|  Research Assistant|    61203.0|\n",
      "|       Germany|      Data Scientist|   134020.0|\n",
      "|        Brazil|  Research Assistant|    71191.0|\n",
      "|     Australia| Mechanical Engineer|   109597.0|\n",
      "|        Mexico|               Nurse|    83077.0|\n",
      "|         Japan|       HR Specialist|    79223.0|\n",
      "|     Australia|               Nurse|    80009.0|\n",
      "|       Germany|                NULL|   147362.0|\n",
      "|          NULL|               Nurse|    79867.0|\n",
      "|United Kingdom|       Sales Manager|   115069.0|\n",
      "| United States|   Software Engineer|   175647.0|\n",
      "|        France|                NULL|    86290.0|\n",
      "|       Germany|Marketing Specialist|       NULL|\n",
      "|United Kingdom|        Data Analyst|   224755.0|\n",
      "|          NULL|             Teacher|    59432.0|\n",
      "| United States|               Nurse|    85847.0|\n",
      "| United States|     Product Manager|   111579.0|\n",
      "+--------------+--------------------+-----------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# Business wants to know country wise payments for every occupation\n",
    "people_df.groupby('country','occupation').agg(sum('salary')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaddf7f-5744-4fb8-8424-62ef2087743a",
   "metadata": {},
   "source": [
    "# Lecture 9\n",
    "## Join\n",
    "\n",
    "## Potential Interview questions\n",
    "\n",
    "1) How join works? Matches the ID or specific key to other table and based on the match it will return the data.\n",
    "2) Why do we need joins? ---> If some data is needed and it is not in single table , then joins can be used to get the data from two or more tables. \n",
    "3) What do we do after joining tow tables ? -- see if the table explodes if it is then make sure to use distinct \n",
    "4) What if two tables have same column name ? -- Ambigious error / mention df and then column then it willnot error out\n",
    "5) Join on tow or more columns ?  df1.join(df2, ((expr1) & (expr2)), jointype)\n",
    "6) Types of join?  7 - inner, outer, left, right, left anti, left semi, cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cde1bd61-cc77-4184-b06a-35f92c3c35b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Not sure why it is throwing error\n",
    "# # Creating demo data for join operation:\n",
    "\n",
    "\n",
    "\n",
    "# # -------------------------------\n",
    "# # 1️⃣ Customer DataFrame\n",
    "# # -------------------------------\n",
    "# customer_data = [\n",
    "#     (1,'manish','patna',\"30-05-2022\"),\n",
    "#     (2,'vikash','kolkata',\"12-03-2023\"),\n",
    "#     (3,'nikita','delhi',\"25-06-2023\"),\n",
    "#     (4,'rahul','ranchi',\"24-03-2023\"),\n",
    "#     (5,'mahesh','jaipur',\"22-03-2023\"),\n",
    "#     (6,'prantosh','kolkata',\"18-10-2022\"),\n",
    "#     (7,'raman','patna',\"30-12-2022\"),\n",
    "#     (8,'prakash','ranchi',\"24-02-2023\"),\n",
    "#     (9,'ragini','kolkata',\"03-03-2023\"),\n",
    "#     (10,'raushan','jaipur',\"05-02-2023\")\n",
    "# ]\n",
    "\n",
    "# customer_schema = ['customer_id','customer_name','address','date_of_joining']\n",
    "\n",
    "# customer_df = spark.createDataFrame(customer_data, schema=customer_schema)\n",
    "\n",
    "# # -------------------------------\n",
    "# # 2️⃣ Sales DataFrame\n",
    "# # -------------------------------\n",
    "# sales_data = [\n",
    "#     (1,22,10,\"01-06-2022\"),\n",
    "#     (1,27,5,\"03-02-2023\"),\n",
    "#     (2,5,3,\"01-06-2023\"),\n",
    "#     (5,22,1,\"22-03-2023\"),\n",
    "#     (7,22,4,\"03-02-2023\"),\n",
    "#     (9,5,6,\"03-03-2023\"),\n",
    "#     (2,1,12,\"15-06-2023\"),\n",
    "#     (1,56,2,\"25-06-2023\"),\n",
    "#     (5,12,5,\"15-04-2023\"),\n",
    "#     (11,12,76,\"12-03-2023\")\n",
    "# ]\n",
    "\n",
    "# sales_schema = ['customer_id','product_id','quantity','date_of_purchase']\n",
    "\n",
    "# sales_df = spark.createDataFrame(sales_data, schema=sales_schema)\n",
    "\n",
    "# # -------------------------------\n",
    "# # 3️⃣ Product DataFrame\n",
    "# # -------------------------------\n",
    "# product_data = [\n",
    "#     (1, 'fanta',20),\n",
    "#     (2, 'dew',22),\n",
    "#     (5, 'sprite',40),\n",
    "#     (7, 'redbull',100),\n",
    "#     (12,'mazza',45),\n",
    "#     (22,'coke',27),\n",
    "#     (25,'limca',21),\n",
    "#     (27,'pepsi',14),\n",
    "#     (56,'sting',10)\n",
    "# ]\n",
    "\n",
    "# product_schema = ['id','name','price']\n",
    "\n",
    "# product_df = spark.createDataFrame(product_data, schema=product_schema)\n",
    "\n",
    "# # -------------------------------\n",
    "# customer_df.show()\n",
    "# sales_df.show()\n",
    "# product_df.show()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "46520165-7cd1-4e5d-b6a6-e674bdfb4c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join syntax\n",
    "\n",
    "#df1.join(df2, expression, jointype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2f2a5f-9a5f-4156-a06d-9079375ded06",
   "metadata": {},
   "source": [
    "# Lecture 10 \n",
    "## Join Strategy\n",
    "\n",
    "## Potential interview questions \n",
    "\n",
    "1) what are the join strategy in Spark\n",
    "       A. Shuffle sort-merge\n",
    "       B. Shuffle hash join\n",
    "       C. Broadcast Jpoin\n",
    "       D. Cartesion join\n",
    "       E. Broadcast nested loop join  \n",
    "3) Why joins are expensive/ wide dependency transformation ?\n",
    "4) Difference between shuffle hash join and sort merge join ?\n",
    "5) when do we need brodcast join ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e685a20c-ec00-461d-908d-15d6ebcc8cd7",
   "metadata": {},
   "source": [
    "# Lecture 11\n",
    "## Broadcast Hash join \n",
    "\n",
    "## Potential Interview questions \n",
    "1) why do we need broadcast hash join - Basically to avoide shuffeling\n",
    "2) How does broadcast join works - Driver will broadcast the smaller table to all executors\n",
    "3) Difference between broadcast hash join and shuffle hash join - hash tere will be shuffeling , broadcast there will not be shuffeling\n",
    "4) How can we change the broadcast size of the table - set it while creating session\n",
    "5) When the broadcast join will fail  - when the size of the table is too big \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5076b877-4943-4230-be15-53a8fa09a6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntax for broad cast join \n",
    "#Broadcast_df =  df.join(broadcast(df2), (expression))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f072e612-790f-4829-9515-86c037028211",
   "metadata": {},
   "source": [
    "# lecture 12 \n",
    "## Window Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f80b9c60-5a15-47ac-818f-0064e83b3376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# window = window.partitionBy('dept').orderBy('salary')\n",
    "\n",
    "# df = df.withcolumn('RowNumner',rownumber().over(window))\\\n",
    "#        .withcolumn('DenseRank',denserank().over(window))\\\n",
    "#        .withcolumn('Rank',rank().over(window))\\\n",
    "#        .show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1541d3-5853-44ba-96da-529bac4c41c9",
   "metadata": {},
   "source": [
    "# Lecture 13 Lead and Lag \n",
    "## Potential interview questions\n",
    "\n",
    "1) What is lead ? \n",
    "2) What is lag ? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "118f7d2b-0f40-4bf6-a099-640dd1d358a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+-----+\n",
      "|product_id|product_name|sales_date|sales|\n",
      "+----------+------------+----------+-----+\n",
      "|      P001| Apex Widget|2025-06-01|  106|\n",
      "|      P001| Apex Widget|2025-06-02|  100|\n",
      "|      P001| Apex Widget|2025-06-03|  111|\n",
      "|      P001| Apex Widget|2025-06-04|  123|\n",
      "|      P001| Apex Widget|2025-06-05|  103|\n",
      "|      P001| Apex Widget|2025-06-06|  104|\n",
      "|      P001| Apex Widget|2025-06-07|  127|\n",
      "|      P001| Apex Widget|2025-06-08|  119|\n",
      "|      P001| Apex Widget|2025-06-09|  106|\n",
      "|      P001| Apex Widget|2025-06-10|  119|\n",
      "|      P001| Apex Widget|2025-06-11|  109|\n",
      "|      P001| Apex Widget|2025-06-12|  110|\n",
      "|      P001| Apex Widget|2025-06-13|  120|\n",
      "|      P001| Apex Widget|2025-06-14|   96|\n",
      "|      P001| Apex Widget|2025-06-15|   99|\n",
      "|      P001| Apex Widget|2025-06-16|  115|\n",
      "|      P001| Apex Widget|2025-06-17|  111|\n",
      "|      P001| Apex Widget|2025-06-18|  128|\n",
      "|      P001| Apex Widget|2025-06-19|  115|\n",
      "|      P001| Apex Widget|2025-06-20|  110|\n",
      "|      P002|   Bolt Plus|2025-06-01|   79|\n",
      "|      P002|   Bolt Plus|2025-06-02|   62|\n",
      "|      P002|   Bolt Plus|2025-06-03|   69|\n",
      "|      P002|   Bolt Plus|2025-06-04|   54|\n",
      "|      P002|   Bolt Plus|2025-06-05|   69|\n",
      "|      P002|   Bolt Plus|2025-06-06|   80|\n",
      "|      P002|   Bolt Plus|2025-06-07|   68|\n",
      "|      P002|   Bolt Plus|2025-06-08|   90|\n",
      "|      P002|   Bolt Plus|2025-06-09|   82|\n",
      "|      P002|   Bolt Plus|2025-06-10|   89|\n",
      "+----------+------------+----------+-----+\n",
      "only showing top 30 rows\n"
     ]
    }
   ],
   "source": [
    "# lets read a df first\n",
    "\n",
    "sales_df = spark.read.format('csv')\\\n",
    "            .option('header','true')\\\n",
    "            .option('inferschema','true')\\\n",
    "            .option('mode','permissive')\\\n",
    "            .load('sales_window_data.csv')\n",
    "\n",
    "sales_df.show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "79c2eb99-6ebb-4e24-94ad-64d8795de490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+-----+--------------------+\n",
      "|product_id|product_name|sales_date|sales|Previous_Month_Sales|\n",
      "+----------+------------+----------+-----+--------------------+\n",
      "|      P001| Apex Widget|2025-06-01|  106|                NULL|\n",
      "|      P001| Apex Widget|2025-06-02|  100|                 106|\n",
      "|      P001| Apex Widget|2025-06-03|  111|                 100|\n",
      "|      P001| Apex Widget|2025-06-04|  123|                 111|\n",
      "|      P001| Apex Widget|2025-06-05|  103|                 123|\n",
      "|      P001| Apex Widget|2025-06-06|  104|                 103|\n",
      "|      P001| Apex Widget|2025-06-07|  127|                 104|\n",
      "|      P001| Apex Widget|2025-06-08|  119|                 127|\n",
      "|      P001| Apex Widget|2025-06-09|  106|                 119|\n",
      "|      P001| Apex Widget|2025-06-10|  119|                 106|\n",
      "|      P001| Apex Widget|2025-06-11|  109|                 119|\n",
      "|      P001| Apex Widget|2025-06-12|  110|                 109|\n",
      "|      P001| Apex Widget|2025-06-13|  120|                 110|\n",
      "|      P001| Apex Widget|2025-06-14|   96|                 120|\n",
      "|      P001| Apex Widget|2025-06-15|   99|                  96|\n",
      "|      P001| Apex Widget|2025-06-16|  115|                  99|\n",
      "|      P001| Apex Widget|2025-06-17|  111|                 115|\n",
      "|      P001| Apex Widget|2025-06-18|  128|                 111|\n",
      "|      P001| Apex Widget|2025-06-19|  115|                 128|\n",
      "|      P001| Apex Widget|2025-06-20|  110|                 115|\n",
      "+----------+------------+----------+-----+--------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# first lets get window for product ID and order it by date \n",
    "# then get pervious month sales \n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import lag, lead, col\n",
    "\n",
    "\n",
    "window = Window.partitionBy('product_id').orderBy('sales_date')\n",
    "\n",
    "# now lets calculate previous month sales \n",
    "previous_month_sales = sales_df.withColumn('Previous_Month_Sales',lag(col('sales'),1).over(window))\n",
    "previous_month_sales.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6977b184-35a1-45bd-817b-96e3c2077852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+-----+----------------+\n",
      "|product_id|product_name|sales_date|sales|Next_Month_Sales|\n",
      "+----------+------------+----------+-----+----------------+\n",
      "|      P001| Apex Widget|2025-06-01|  106|             100|\n",
      "|      P001| Apex Widget|2025-06-02|  100|             111|\n",
      "|      P001| Apex Widget|2025-06-03|  111|             123|\n",
      "|      P001| Apex Widget|2025-06-04|  123|             103|\n",
      "|      P001| Apex Widget|2025-06-05|  103|             104|\n",
      "|      P001| Apex Widget|2025-06-06|  104|             127|\n",
      "|      P001| Apex Widget|2025-06-07|  127|             119|\n",
      "|      P001| Apex Widget|2025-06-08|  119|             106|\n",
      "|      P001| Apex Widget|2025-06-09|  106|             119|\n",
      "|      P001| Apex Widget|2025-06-10|  119|             109|\n",
      "|      P001| Apex Widget|2025-06-11|  109|             110|\n",
      "|      P001| Apex Widget|2025-06-12|  110|             120|\n",
      "|      P001| Apex Widget|2025-06-13|  120|              96|\n",
      "|      P001| Apex Widget|2025-06-14|   96|              99|\n",
      "|      P001| Apex Widget|2025-06-15|   99|             115|\n",
      "|      P001| Apex Widget|2025-06-16|  115|             111|\n",
      "|      P001| Apex Widget|2025-06-17|  111|             128|\n",
      "|      P001| Apex Widget|2025-06-18|  128|             115|\n",
      "|      P001| Apex Widget|2025-06-19|  115|             110|\n",
      "|      P001| Apex Widget|2025-06-20|  110|            NULL|\n",
      "+----------+------------+----------+-----+----------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# lets calculate next month sales\n",
    "\n",
    "next_month_sales = sales_df.withColumn('Next_Month_Sales',lead(col('sales'),1).over(window))\n",
    "next_month_sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "30ad7e90-2230-481e-88fb-71be6018365f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+-----+--------------------+--------+\n",
      "|product_id|product_name|sales_date|sales|Previous_Month_Sales|% Change|\n",
      "+----------+------------+----------+-----+--------------------+--------+\n",
      "|      P001| Apex Widget|2025-06-01|  106|                NULL|    NULL|\n",
      "|      P001| Apex Widget|2025-06-02|  100|                 106|   -5.66|\n",
      "|      P001| Apex Widget|2025-06-03|  111|                 100|    11.0|\n",
      "|      P001| Apex Widget|2025-06-04|  123|                 111|   10.81|\n",
      "|      P001| Apex Widget|2025-06-05|  103|                 123|  -16.26|\n",
      "|      P001| Apex Widget|2025-06-06|  104|                 103|    0.97|\n",
      "|      P001| Apex Widget|2025-06-07|  127|                 104|   22.12|\n",
      "|      P001| Apex Widget|2025-06-08|  119|                 127|    -6.3|\n",
      "|      P001| Apex Widget|2025-06-09|  106|                 119|  -10.92|\n",
      "|      P001| Apex Widget|2025-06-10|  119|                 106|   12.26|\n",
      "|      P001| Apex Widget|2025-06-11|  109|                 119|    -8.4|\n",
      "|      P001| Apex Widget|2025-06-12|  110|                 109|    0.92|\n",
      "|      P001| Apex Widget|2025-06-13|  120|                 110|    9.09|\n",
      "|      P001| Apex Widget|2025-06-14|   96|                 120|   -20.0|\n",
      "|      P001| Apex Widget|2025-06-15|   99|                  96|    3.13|\n",
      "|      P001| Apex Widget|2025-06-16|  115|                  99|   16.16|\n",
      "|      P001| Apex Widget|2025-06-17|  111|                 115|   -3.48|\n",
      "|      P001| Apex Widget|2025-06-18|  128|                 111|   15.32|\n",
      "|      P001| Apex Widget|2025-06-19|  115|                 128|  -10.16|\n",
      "|      P001| Apex Widget|2025-06-20|  110|                 115|   -4.35|\n",
      "+----------+------------+----------+-----+--------------------+--------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# now how to calculate % increase of decrease \n",
    "# ((sales - previousMonthsales)/sales)*100\n",
    "from pyspark.sql.functions import col, round\n",
    "\n",
    "\n",
    "percentage_sales = previous_month_sales.withColumn('% Change', round(((col('sales') - col('Previous_Month_Sales')) / col('Previous_Month_Sales')) * 100, 2)).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6213974-5768-4418-8509-52369a2185ce",
   "metadata": {},
   "source": [
    "# Lecture 14 : Flatten Json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0a473e3b-b14c-4d49-8d75-e66b6c71021c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+--------------------+-------------+-------------+-------------+------+\n",
      "|code|message|         restaurants|results_found|results_shown|results_start|status|\n",
      "+----+-------+--------------------+-------------+-------------+-------------+------+\n",
      "|NULL|   NULL|                  []|            0|            0|            1|  NULL|\n",
      "|NULL|   NULL|[{{{17066603}, b9...|         6835|           20|            1|  NULL|\n",
      "|NULL|   NULL|                  []|            0|            0|            1|  NULL|\n",
      "|NULL|   NULL|                  []|            0|            0|            1|  NULL|\n",
      "|NULL|   NULL|[{{{17093124}, b9...|         8680|           20|            1|  NULL|\n",
      "|NULL|   NULL|                  []|            0|            0|            1|  NULL|\n",
      "|NULL|   NULL|                  []|            0|            0|            1|  NULL|\n",
      "|NULL|   NULL|[{{{17580142}, b9...|          943|           20|            1|  NULL|\n",
      "|NULL|   NULL|                  []|            0|            0|            1|  NULL|\n",
      "|NULL|   NULL|                  []|            0|            0|            1|  NULL|\n",
      "|NULL|   NULL|[{{{17284158}, b9...|          257|           20|            1|  NULL|\n",
      "|NULL|   NULL|                  []|            0|            0|            1|  NULL|\n",
      "|NULL|   NULL|                  []|            0|            0|            1|  NULL|\n",
      "|NULL|   NULL|[{{{17678233}, b9...|          358|           20|            1|  NULL|\n",
      "|NULL|   NULL|                  []|            0|            0|            1|  NULL|\n",
      "|NULL|   NULL|                  []|            0|            0|            1|  NULL|\n",
      "|NULL|   NULL|[{{{17375047}, b9...|          641|           20|            1|  NULL|\n",
      "|NULL|   NULL|                  []|            0|            0|            1|  NULL|\n",
      "|NULL|   NULL|                  []|            0|            0|            1|  NULL|\n",
      "|NULL|   NULL|[{{{17616590}, b9...|         1613|           20|            1|  NULL|\n",
      "+----+-------+--------------------+-------------+-------------+-------------+------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# read json file\n",
    "\n",
    "restaurent_Data = spark.read.format('json')\\\n",
    "                .option('multiline','true')\\\n",
    "                .option('inferschema','true')\\\n",
    "                .load('resturant_json_data.json')\n",
    "\n",
    "restaurent_Data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3db01f77-8713-491b-a058-b9a1e4829fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- code: long (nullable = true)\n",
      " |-- message: string (nullable = true)\n",
      " |-- restaurants: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- restaurant: struct (nullable = true)\n",
      " |    |    |    |-- R: struct (nullable = true)\n",
      " |    |    |    |    |-- res_id: long (nullable = true)\n",
      " |    |    |    |-- apikey: string (nullable = true)\n",
      " |    |    |    |-- average_cost_for_two: long (nullable = true)\n",
      " |    |    |    |-- cuisines: string (nullable = true)\n",
      " |    |    |    |-- currency: string (nullable = true)\n",
      " |    |    |    |-- deeplink: string (nullable = true)\n",
      " |    |    |    |-- establishment_types: array (nullable = true)\n",
      " |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |-- events_url: string (nullable = true)\n",
      " |    |    |    |-- featured_image: string (nullable = true)\n",
      " |    |    |    |-- has_online_delivery: long (nullable = true)\n",
      " |    |    |    |-- has_table_booking: long (nullable = true)\n",
      " |    |    |    |-- id: string (nullable = true)\n",
      " |    |    |    |-- is_delivering_now: long (nullable = true)\n",
      " |    |    |    |-- location: struct (nullable = true)\n",
      " |    |    |    |    |-- address: string (nullable = true)\n",
      " |    |    |    |    |-- city: string (nullable = true)\n",
      " |    |    |    |    |-- city_id: long (nullable = true)\n",
      " |    |    |    |    |-- country_id: long (nullable = true)\n",
      " |    |    |    |    |-- latitude: string (nullable = true)\n",
      " |    |    |    |    |-- locality: string (nullable = true)\n",
      " |    |    |    |    |-- locality_verbose: string (nullable = true)\n",
      " |    |    |    |    |-- longitude: string (nullable = true)\n",
      " |    |    |    |    |-- zipcode: string (nullable = true)\n",
      " |    |    |    |-- menu_url: string (nullable = true)\n",
      " |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |-- offers: array (nullable = true)\n",
      " |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |-- photos_url: string (nullable = true)\n",
      " |    |    |    |-- price_range: long (nullable = true)\n",
      " |    |    |    |-- switch_to_order_menu: long (nullable = true)\n",
      " |    |    |    |-- thumb: string (nullable = true)\n",
      " |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |    |-- user_rating: struct (nullable = true)\n",
      " |    |    |    |    |-- aggregate_rating: string (nullable = true)\n",
      " |    |    |    |    |-- rating_color: string (nullable = true)\n",
      " |    |    |    |    |-- rating_text: string (nullable = true)\n",
      " |    |    |    |    |-- votes: string (nullable = true)\n",
      " |-- results_found: long (nullable = true)\n",
      " |-- results_shown: long (nullable = true)\n",
      " |-- results_start: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "restaurent_Data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "46b6cc99-772f-4c24-a5c9-aad555ec7c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- code: long (nullable = true)\n",
      " |-- message: string (nullable = true)\n",
      " |-- restaurants: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- restaurant: struct (nullable = true)\n",
      " |    |    |    |-- R: struct (nullable = true)\n",
      " |    |    |    |    |-- res_id: long (nullable = true)\n",
      " |    |    |    |-- apikey: string (nullable = true)\n",
      " |    |    |    |-- average_cost_for_two: long (nullable = true)\n",
      " |    |    |    |-- cuisines: string (nullable = true)\n",
      " |    |    |    |-- currency: string (nullable = true)\n",
      " |    |    |    |-- deeplink: string (nullable = true)\n",
      " |    |    |    |-- establishment_types: array (nullable = true)\n",
      " |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |-- events_url: string (nullable = true)\n",
      " |    |    |    |-- featured_image: string (nullable = true)\n",
      " |    |    |    |-- has_online_delivery: long (nullable = true)\n",
      " |    |    |    |-- has_table_booking: long (nullable = true)\n",
      " |    |    |    |-- id: string (nullable = true)\n",
      " |    |    |    |-- is_delivering_now: long (nullable = true)\n",
      " |    |    |    |-- location: struct (nullable = true)\n",
      " |    |    |    |    |-- address: string (nullable = true)\n",
      " |    |    |    |    |-- city: string (nullable = true)\n",
      " |    |    |    |    |-- city_id: long (nullable = true)\n",
      " |    |    |    |    |-- country_id: long (nullable = true)\n",
      " |    |    |    |    |-- latitude: string (nullable = true)\n",
      " |    |    |    |    |-- locality: string (nullable = true)\n",
      " |    |    |    |    |-- locality_verbose: string (nullable = true)\n",
      " |    |    |    |    |-- longitude: string (nullable = true)\n",
      " |    |    |    |    |-- zipcode: string (nullable = true)\n",
      " |    |    |    |-- menu_url: string (nullable = true)\n",
      " |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |-- offers: array (nullable = true)\n",
      " |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |-- photos_url: string (nullable = true)\n",
      " |    |    |    |-- price_range: long (nullable = true)\n",
      " |    |    |    |-- switch_to_order_menu: long (nullable = true)\n",
      " |    |    |    |-- thumb: string (nullable = true)\n",
      " |    |    |    |-- url: string (nullable = true)\n",
      " |    |    |    |-- user_rating: struct (nullable = true)\n",
      " |    |    |    |    |-- aggregate_rating: string (nullable = true)\n",
      " |    |    |    |    |-- rating_color: string (nullable = true)\n",
      " |    |    |    |    |-- rating_text: string (nullable = true)\n",
      " |    |    |    |    |-- votes: string (nullable = true)\n",
      " |-- results_found: long (nullable = true)\n",
      " |-- results_shown: long (nullable = true)\n",
      " |-- results_start: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- restaurants_new: struct (nullable = true)\n",
      " |    |-- restaurant: struct (nullable = true)\n",
      " |    |    |-- R: struct (nullable = true)\n",
      " |    |    |    |-- res_id: long (nullable = true)\n",
      " |    |    |-- apikey: string (nullable = true)\n",
      " |    |    |-- average_cost_for_two: long (nullable = true)\n",
      " |    |    |-- cuisines: string (nullable = true)\n",
      " |    |    |-- currency: string (nullable = true)\n",
      " |    |    |-- deeplink: string (nullable = true)\n",
      " |    |    |-- establishment_types: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- events_url: string (nullable = true)\n",
      " |    |    |-- featured_image: string (nullable = true)\n",
      " |    |    |-- has_online_delivery: long (nullable = true)\n",
      " |    |    |-- has_table_booking: long (nullable = true)\n",
      " |    |    |-- id: string (nullable = true)\n",
      " |    |    |-- is_delivering_now: long (nullable = true)\n",
      " |    |    |-- location: struct (nullable = true)\n",
      " |    |    |    |-- address: string (nullable = true)\n",
      " |    |    |    |-- city: string (nullable = true)\n",
      " |    |    |    |-- city_id: long (nullable = true)\n",
      " |    |    |    |-- country_id: long (nullable = true)\n",
      " |    |    |    |-- latitude: string (nullable = true)\n",
      " |    |    |    |-- locality: string (nullable = true)\n",
      " |    |    |    |-- locality_verbose: string (nullable = true)\n",
      " |    |    |    |-- longitude: string (nullable = true)\n",
      " |    |    |    |-- zipcode: string (nullable = true)\n",
      " |    |    |-- menu_url: string (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- offers: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- photos_url: string (nullable = true)\n",
      " |    |    |-- price_range: long (nullable = true)\n",
      " |    |    |-- switch_to_order_menu: long (nullable = true)\n",
      " |    |    |-- thumb: string (nullable = true)\n",
      " |    |    |-- url: string (nullable = true)\n",
      " |    |    |-- user_rating: struct (nullable = true)\n",
      " |    |    |    |-- aggregate_rating: string (nullable = true)\n",
      " |    |    |    |-- rating_color: string (nullable = true)\n",
      " |    |    |    |-- rating_text: string (nullable = true)\n",
      " |    |    |    |-- votes: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# explode elements that you want \n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "restaurent_Data.select('*',explode('restaurants').alias('restaurants_new')).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151d119a-e283-475c-b83a-5cf1c2ded352",
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurent_Data.select('*',explode('restaurants').alias('restaurants_new')).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eeb1e08c-7f1c-498f-96cb-e0a3f2cb7f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- code: long (nullable = true)\n",
      " |-- message: string (nullable = true)\n",
      " |-- results_found: long (nullable = true)\n",
      " |-- results_shown: long (nullable = true)\n",
      " |-- results_start: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- res_id: long (nullable = true)\n",
      " |-- new_establishment_types: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "restaurent_Data.select('*', explode('restaurants').alias('restaurants_new')) \\\n",
    "    .drop('restaurants') \\\n",
    "    .select('*','restaurants_new.restaurant.R.res_id',\n",
    "            explode('restaurants_new.restaurant.establishment_types').alias('new_establishment_types')\n",
    "           , 'restaurants_new.restaurant.name').drop('restaurants_new').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e0be41e8-45e4-4e61-9195-f80f2809302d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-------------+-------------+-------------+------+--------+-----------------------+------------------------------------+\n",
      "|code|message|results_found|results_shown|results_start|status|res_id  |new_establishment_types|name                                |\n",
      "+----+-------+-------------+-------------+-------------+------+--------+-----------------------+------------------------------------+\n",
      "|NULL|NULL   |6835         |20           |1            |NULL  |17066603|NULL                   |The Coop                            |\n",
      "|NULL|NULL   |6835         |20           |1            |NULL  |17059541|NULL                   |Maggiano's Little Italy             |\n",
      "|NULL|NULL   |6835         |20           |1            |NULL  |17064405|NULL                   |Tako Cheena by Pom Pom              |\n",
      "|NULL|NULL   |6835         |20           |1            |NULL  |17057797|NULL                   |Bosphorous Turkish Cuisine          |\n",
      "|NULL|NULL   |6835         |20           |1            |NULL  |17057591|NULL                   |Bahama Breeze Island Grille         |\n",
      "|NULL|NULL   |6835         |20           |1            |NULL  |17064266|NULL                   |Hawkers Asian Street Fare           |\n",
      "|NULL|NULL   |6835         |20           |1            |NULL  |17060516|NULL                   |Seasons 52 Fresh Grill              |\n",
      "|NULL|NULL   |6835         |20           |1            |NULL  |17060320|NULL                   |Raglan Road Irish Pub and Restaurant|\n",
      "|NULL|NULL   |6835         |20           |1            |NULL  |17059060|NULL                   |Hillstone                           |\n",
      "|NULL|NULL   |6835         |20           |1            |NULL  |17059012|NULL                   |Hollerbach's Willow Tree Café       |\n",
      "|NULL|NULL   |6835         |20           |1            |NULL  |17060869|NULL                   |Texas de Brazil                     |\n",
      "|NULL|NULL   |6835         |20           |1            |NULL  |17061231|NULL                   |The Ravenous Pig                    |\n",
      "|NULL|NULL   |6835         |20           |1            |NULL  |17058534|NULL                   |Earl of Sandwich                    |\n",
      "|NULL|NULL   |6835         |20           |1            |NULL  |17057925|NULL                   |Café Tu Tu Tango                    |\n",
      "|NULL|NULL   |6835         |20           |1            |NULL  |17064031|NULL                   |Tibby's New Orleans Kitchen         |\n",
      "|NULL|NULL   |6835         |20           |1            |NULL  |17061237|NULL                   |Cevíche Tapas Bar & Restaurant      |\n",
      "|NULL|NULL   |6835         |20           |1            |NULL  |17061253|NULL                   |Ethos Vegan Kitchen                 |\n",
      "|NULL|NULL   |6835         |20           |1            |NULL  |17061296|NULL                   |Pom Pom's Teahouse and Sandwicheria |\n",
      "|NULL|NULL   |6835         |20           |1            |NULL  |17061205|NULL                   |Yellow Dog Eats                     |\n",
      "|NULL|NULL   |6835         |20           |1            |NULL  |17057397|NULL                   |'Ohana                              |\n",
      "+----+-------+-------------+-------------+-------------+------+--------+-----------------------+------------------------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode_outer, col\n",
    "\n",
    "restaurent_Data.select('*', explode('restaurants').alias('restaurants_new')) \\\n",
    "    .drop('restaurants') \\\n",
    "    .select('*','restaurants_new.restaurant.R.res_id',\n",
    "            explode_outer('restaurants_new.restaurant.establishment_types').alias('new_establishment_types')\n",
    "           , 'restaurants_new.restaurant.name').drop('restaurants_new').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b620c22a-be9e-4e0b-b64c-63c3027fac82",
   "metadata": {},
   "source": [
    "# Lecture 15 : Spark Submit \n",
    "\n",
    "## Potential Interview Questions\n",
    "1) What is spark submit ?\n",
    "2) How do you run your job ob spark cluster ?\n",
    "3) Where is your spark cluster ?\n",
    "4) What is deploy mode in spark cluster ?\n",
    "5) What is master in spark submit ?\n",
    "6) How do you provide memory config and why do you use that much memory ?\n",
    "7) How do you update configuration like broadcast, threshold, timeouot, dynamic etc .. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "2e6ed219-e883-412a-b258-c14955cd1bc2",
   "metadata": {},
   "source": [
    "1) Spark submit is command like tool, which runs spark applications on cluster.\n",
    "It will occumulate every file and needed details and make a package and run it on cluster.\n",
    "\n",
    "2) We trigger our application through spark submit on the cluster.\n",
    "\n",
    "3) standalone, local mode, kuberneteas, yarn \n",
    "\n",
    "4) Client , cluster \n",
    "\n",
    "5) master is where your cluster is runnint (3)\n",
    "\n",
    "6) in congif file \n",
    "\n",
    "7) In config file \n",
    "\n",
    "\n",
    "# ===== Core / Execution =====\n",
    "spark.app.name                       my-spark-job\n",
    "spark.master                         local[*]                     # <— override in CLI for YARN/K8s\n",
    "spark.deploy.mode                    client                       # client or cluster (override in CLI)\n",
    "spark.serializer                     org.apache.spark.serializer.KryoSerializer\n",
    "\n",
    "# ===== Dynamic Allocation (enable when using YARN/K8s/Standalone with shuffle tracking) =====\n",
    "spark.dynamicAllocation.enabled      true\n",
    "spark.dynamicAllocation.minExecutors 1\n",
    "spark.dynamicAllocation.maxExecutors 10\n",
    "# For YARN/Standalone (pre Spark 3.3) you usually needed:\n",
    "# spark.shuffle.service.enabled     true\n",
    "# Since Spark 3.3+, shuffle tracking can replace external shuffle service:\n",
    "spark.dynamicAllocation.shuffleTracking.enabled true\n",
    "\n",
    "# ===== SQL / Joins / Shuffle =====\n",
    "spark.sql.adaptive.enabled           true                         # AQE: auto optimize joins/shuffles\n",
    "spark.sql.shuffle.partitions         200                          # tune to your data/cluster\n",
    "spark.sql.broadcastTimeout           3600                         # seconds; avoid timeouts on large broadcasts\n",
    "spark.sql.autoBroadcastJoinThreshold 100m                         # broadcast small side up to 100 MB\n",
    "\n",
    "# ===== Resources =====\n",
    "spark.driver.memory                  1g\n",
    "spark.driver.cores                   1\n",
    "spark.executor.memory                2g\n",
    "spark.executor.cores                 2\n",
    "# If you prefer fixed executors instead of dynamic allocation, set the next line and disable DA above:\n",
    "# spark.executor.instances           5\n",
    "\n",
    "# ===== Networking / Stability =====\n",
    "spark.network.timeout                800s                         # covers heartbeat & long GC pauses\n",
    "spark.executor.heartbeatInterval     60s\n",
    "\n",
    "# ===== Logging / History (optional but useful) =====\n",
    "spark.eventLog.enabled               true\n",
    "spark.eventLog.dir                   file:///tmp/spark-events     # or hdfs:///… or s3a://…\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228fd23f-5535-4828-9413-370dfcb9c306",
   "metadata": {},
   "source": [
    "# Lecture 16 : Deployment Mode in Spark \n",
    "## Potential Interview Questions? \n",
    "\n",
    "1) Whar are all deployment modes in spark ?\n",
    "2) What is edge node ?\n",
    "3) Why do we need client and cluster modes ?\n",
    "4) What will happen if i close my edge node ?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f99a9f3a-6847-4426-92eb-04e3a2d6124c",
   "metadata": {},
   "source": [
    "1) Deployment modes in Spark:\n",
    "Cluster managers: local, Standalone, YARN, Kubernetes; deploy modes: client and cluster (where the driver runs).\n",
    "\n",
    "2) What is an edge node?\n",
    "A gateway machine you submit jobs from—has tools and cluster access but doesn’t run executors.\n",
    "\n",
    "3) Why client vs cluster modes?\n",
    "To choose driver placement: client for interactive/debugging (driver on submit node), cluster for production resiliency/low latency (driver on cluster).\n",
    "\n",
    "4) If you close the edge node?\n",
    "Client mode: job dies; Cluster mode: job continues running on the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e22530-d7e1-45ad-adf9-1fa141bb05ea",
   "metadata": {},
   "source": [
    "# Spark Deploy Modes — Client vs Cluster\n",
    "\n",
    "| Aspect | Client mode | Cluster mode |\n",
    "|---|---|---|\n",
    "| Where the driver runs | On the submit node (your laptop/edge node) | On the cluster (YARN AM / K8s driver pod / Standalone worker) |\n",
    "| If submit node disconnects | **Job dies** (driver is gone) | **Job keeps running** on the cluster |\n",
    "| Driver logs | Your terminal (stdout/stderr) & local logs | Cluster logs (YARN container logs / `kubectl logs` / Standalone worker logs) |\n",
    "| Network latency | Higher (driver far from executors) | Lower (driver near executors) |\n",
    "| Driver OOM risk | Possible; sized on your machine | Possible; sized on cluster; often more stable |\n",
    "| Best for | Dev, ad-hoc, interactive, easy local debugging | Production, scheduled jobs, resiliency, central logs |\n",
    "| Example run | `spark-submit --master yarn --deploy-mode client app.jar` | `spark-submit --master yarn --deploy-mode cluster app.jar` |\n",
    "| Fetch logs | (local terminal/files) | YARN: `yarn logs -applicationId <app_id>` · K8s: `kubectl logs <driver-pod>` |\n",
    "\n",
    "### Handy configs (both modes)\n",
    "- `--driver-memory 2g  --driver-cores 2`\n",
    "- `--conf spark.dynamicAllocation.enabled=true`\n",
    "- `--conf spark.dynamicAllocation.minExecutors=1`\n",
    "- `--conf spark.dynamicAllocation.maxExecutors=10`\n",
    "- `--conf spark.sql.adaptive.enabled=true`\n",
    "- `--conf spark.sql.autoBroadcastJoinThreshold=100m`\n",
    "- `--conf spark.sql.broadcastTimeout=3600`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbbf0c8-4136-4c35-b57e-daad1b2ba001",
   "metadata": {},
   "source": [
    "# Lecture 18\n",
    "\n",
    "## Potential Interview Questions\n",
    "1) What is AQE ?\n",
    "2) Why do we need AQE ?\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299da949-f90e-476d-b0bf-7aea735116ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "1) Allow to change the query while runtime \n",
    "2) Cnahge query dynamically while run time \n",
    "\n",
    "Above spark 3.0 and above \n",
    "\n",
    "Features of AQE:\n",
    "Dynamically Coalescing shuffeling partition\n",
    "Dynamically switching join strategies\n",
    "Dynamically Optimizing Skew join \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ba9e8e-9156-47f9-bcc6-abd5a271bc89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c309384-8df2-4ded-ae16-e2c278a45b4b",
   "metadata": {},
   "source": [
    "# Union / Union All / Union By Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0432b2e-77ab-4bd1-9d7f-f0ba0f4a7090",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad287a56-df67-43c3-99b3-7b16fd21684f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True),\n",
    "    StructField(\"dept_id\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# ✅ df1\n",
    "data = [\n",
    "    (10, 'Anil', 50000, 18),\n",
    "    (11, 'Vikas', 75000, 16),\n",
    "    (12, 'Nisha', 40000, 18),\n",
    "    (13, 'Nidhi', 60000, 17),\n",
    "    (14, 'Priya', 80000, 18),\n",
    "    (15, 'Mohit', 45000, 18),\n",
    "    (16, 'Rajesh', 90000, 10),\n",
    "    (17, 'Raman', 55000, 16),\n",
    "    (18, 'Sam', 65000, 17)\n",
    "]\n",
    "df1 = spark.createDataFrame(data, schema)\n",
    "\n",
    "# ✅ df2 (same schema, can be used for union or unionAll)\n",
    "data1 = [\n",
    "    (19, 'Sohan', 50000, 18),\n",
    "    (20, 'Sima', 75000, 17)\n",
    "]\n",
    "df2 = spark.createDataFrame(data1, schema)\n",
    "\n",
    "# ✅ df3_wrong_col_order (same values but different column order, for unionByName)\n",
    "wrong_column_data = [\n",
    "    (19, 50000, 18, 'Sohan'),\n",
    "    (20, 75000, 17, 'Sima')\n",
    "]\n",
    "# Note the changed column order in schema\n",
    "wrong_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True),\n",
    "    StructField(\"dept_id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True)\n",
    "])\n",
    "df3_wrong_col_order = spark.createDataFrame(wrong_column_data, wrong_schema)\n",
    "\n",
    "# ✅ df4_extra_column (extra column → to demonstrate schema mismatch error)\n",
    "extra_column_data = [\n",
    "    (19, 50000, 18, 'Sohan', 10),\n",
    "    (20, 75000, 17, 'Sima', 20)\n",
    "]\n",
    "extra_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True),\n",
    "    StructField(\"dept_id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"bonus\", IntegerType(), True)\n",
    "])\n",
    "df4_extra_column = spark.createDataFrame(extra_column_data, extra_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2844d912-5512-4187-861e-3e8f529fa990",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Union (same schema)\n",
    "# df1.union(df2).show()\n",
    "\n",
    "# # Union By Name (works even if columns are ordered differently)\n",
    "# df1.unionByName(df3_wrong_col_order).show()\n",
    "\n",
    "# # This will cause error: schema mismatch\n",
    "# df1.union(df4_extra_column).show()  # ❌ Error: differing column count\n",
    "# # so in this case we can select column manually and then we can perform the union operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d975bf48-2588-4bf8-b5d3-72dda74d33f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dfca716a-60d2-49c0-b6f8-924b4ae12708",
   "metadata": {},
   "source": [
    "# Repartition and Coalesec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b266c9-a54c-487b-b465-2035e9fa9c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Flight_Data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d1669f-13ce-4408-945b-13a0e4fc8366",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Flight_Data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8191c8-f3c5-4021-a4d6-ab928c4932e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get the partition we need to convert it to RDD first \n",
    "df_Flight_Data.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d815ff-6690-4371-9d54-99767bf86fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will makle 4 partitions \n",
    "df_Flight_Data_partitions= df_Flight_Data.repartition(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b87a679-0747-48fc-941e-c15fc0991cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how the allocation happned \n",
    "from pyspark.sql.functions import spark_partition_id\n",
    "\n",
    "df_Flight_Data_partitions.withColumn(\"PartitionID\",spark_partition_id()).groupBy(\"PartitionID\").count().show()\n",
    "# Evenly Disrtributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec44bb69-fc37-48ff-9acc-4c2a988aea5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also create partition based on columns and also give number of partitions we need, if there is no value then null will be assigned \n",
    "\n",
    "partitioned_by_column= df_Flight_Data.repartition(300,'ORIGIN_COUNTRY_NAME')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5557b2f5-7457-4b47-9093-976d638d359a",
   "metadata": {},
   "outputs": [],
   "source": [
    "partitioned_by_column.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82337b66-994f-4feb-8fd6-aa769cf8e120",
   "metadata": {},
   "outputs": [],
   "source": [
    "partitioned_by_column.withColumn(\"PartitionID\",spark_partition_id()).groupBy(\"PartitionID\").count().show()\n",
    "# we were having less values 255 and number of parttitions werer 300 , so null in remeaning values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6216c1-09aa-45a7-9f1f-7d8a2fb5a18d",
   "metadata": {},
   "source": [
    "# Coalsence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c81495-812f-44dc-a34d-ac1d9cd97322",
   "metadata": {},
   "outputs": [],
   "source": [
    "coalesce_flight_df = df_Flight_Data.repartition(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d086bb58-0961-4e94-84c6-c19f76fe529d",
   "metadata": {},
   "outputs": [],
   "source": [
    "coalesce_flight_df.withColumn(\"PartitionID\",spark_partition_id()).groupBy(\"PartitionID\").count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47524415-db23-49cf-8949-3da23ed25eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce to 3 \n",
    "\n",
    "three_col_partition = coalesce_flight_df.coalesce(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d9940d-f757-4f6d-a2c0-c2e4c39d0c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "three_col_partition.withColumn(\"PartitionID\",spark_partition_id()).groupBy(\"PartitionID\").count().show()\n",
    "# not evenly distributed, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296a4300-904c-4b80-986c-203ffb6dc6c6",
   "metadata": {},
   "source": [
    "# Case When, When Otherwise/ Null values dealing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d52268d-eea2-4dc7-b55b-33c2e9bfc0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Flight_Data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafe359c-d859-4490-9d4c-95c63de8c12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "df_Flight_Data.withColumn('Frequent Travel', when(col('count') > 100, 'Medium Yes')\n",
    "                                             .when(col('count') > 200,'Too Frequent')\n",
    "                                             .otherwise ('Need to Improve')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a3cb9c-a3e3-4ae6-9921-529fa286f6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# null values \n",
    "\n",
    "\n",
    "from pyspark.sql.functions import when, col, lit\n",
    "\n",
    "df_Flight_Data.withColumn(\n",
    "    'count', \n",
    "    when(col('count').isNull(), lit(1)).otherwise(col('count'))\n",
    ").withColumn(\n",
    "    'Null Details', \n",
    "    when(col('ORIGIN_COUNTRY_NAME') == 'Afghanistan', 'cancle Flight')\n",
    "    .otherwise('KEEP FLIGHT')\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdc174c-94c8-4b6c-a5de-03b944f98965",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Flight_Data.createOrReplaceTempView('FlightSQLWhenOtherWise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0200c91-8899-457d-8242-8bc762f060eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT  \n",
    "  CASE \n",
    "    WHEN ORIGIN_COUNTRY_NAME = 'Afghanistan' THEN 'Cancel Flight'\n",
    "    ELSE 'Keep Flight'\n",
    "  END AS FlightDetails\n",
    "FROM FlightSQLWhenOtherWise\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babb00a0-f07b-44e7-ab72-0f9be3e889e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT  \n",
    "  CASE \n",
    "    WHEN ORIGIN_COUNTRY_NAME = 'Afghanistan' THEN 'Cancel Flight' \n",
    "    WHEN ORIGIN_COUNTRY_NAME = 'Sint Maarten' THEN 'Rerrange Flight'\n",
    "    ELSE 'Keep Flight'\n",
    "  END AS FlightDetails\n",
    "FROM FlightSQLWhenOtherWise\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9477d9f6-b0d5-4b6c-8943-1eb77cc1e8ad",
   "metadata": {},
   "source": [
    "# Unbique Value, Drop Duplicates , Sort Data In Asc and Desc value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcd6aad-6526-4064-8259-757033e9d7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "data = [\n",
    "    (10 ,'Anil',50000, 18),\n",
    "    (11 ,'Vikas',75000, 16),\n",
    "    (12 ,'Nisha',40000, 18),\n",
    "    (13 ,'Nidhi',60000, 17),\n",
    "    (14 ,'Priya',80000, 18),\n",
    "    (15 ,'Mohit',45000, 18),\n",
    "    (16 ,'Rajesh',90000, 10),\n",
    "    (17 ,'Raman',55000, 16),\n",
    "    (18 ,'Sam',65000, 17),\n",
    "    (15 ,'Mohit',45000, 18),\n",
    "    (13 ,'Nidhi',60000, 17),      \n",
    "    (14 ,'Priya',90000, 18),  \n",
    "    (18 ,'Sam',65000, 17)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfc021b-c121-44cc-9b57-d7c7c0943e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the unique values\n",
    "df.distinct().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fd3a79-126c-4f5b-b91e-589f7b64c5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"id\",'name').distinct().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44e2f75-0594-41f7-9578-17dee9f6f4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates\n",
    "df.drop_duplicates(subset=['id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc4ff5d-e3d7-4dbf-857a-06648531e03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort \n",
    "df.sort(col('id'),asc).show()\n",
    "df.sort(col('id'),desc).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af0ea2e-88d3-49e8-ad11-3717c56e6090",
   "metadata": {},
   "source": [
    "# Aggregation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08df7cda-0a25-4476-a62a-b4cd5943c214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count, action and transformation \n",
    "df.select(count('id')).show() # for perticular column\n",
    "df.count() # for all dataframe\n",
    "df.select(count(*))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b067e8-d194-4678-8aaf-96a9ea3e5019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# min\n",
    "df.select(sum('salary').alies ('Total Salary'), max('salary').alies('Max Salary'),min('Salary').alias('Min Salary')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3080a6-af0f-4ee9-a720-af9aa6999a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(sum('salary').alies('Total Salary'),count('salary').alias('Count of Salary'), avg ('salary').alias('AverageSalary')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792259d4-580b-4801-a722-bbb9160e725d",
   "metadata": {},
   "source": [
    "# Group By|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292224bf-c0b0-4cca-8d81-7c6776e45350",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(col('dept').agg(sum('salary').alias('Total Salary')))\n",
    "\n",
    "df.groupBy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc5c63e-bddb-4913-a203-1a5a46adb4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "\n",
    "select Dept, sum(salary) as total_salary\n",
    "from table\n",
    "group by dept\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6e6fff-d027-4b12-8e15-55b8134e817f",
   "metadata": {},
   "source": [
    "# Join "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b0363e-caca-4d16-b746-6debf4fb71ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inner Join \n",
    "\n",
    "df1.join(df2, df1['id']= df2['id'], 'inner').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29d95c9-c816-460b-b70b-a0674bc65316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# left join \n",
    "\n",
    "df1.join(df2, df1['id']=df2['id'],'left').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
